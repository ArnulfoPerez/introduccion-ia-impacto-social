

üß† **Philip K. Dick y su visi√≥n tecnol√≥gica**

Philip K. Dick fue un prol√≠fico autor de ciencia ficci√≥n que escribi√≥ m√°s de 40 novelas y 100 cuentos. Su obra se caracteriza por explorar los l√≠mites entre lo real y lo artificial, la identidad humana, y los efectos psicol√≥gicos y sociales de la tecnolog√≠a. En particular, Dick mostr√≥ una profunda ambivalencia hacia la inteligencia artificial (IA) y los avances tecnol√≥gicos.

üîç **Temas recurrentes en su obra:**
- **Ambig√ºedad entre humanos y androides:** En *Do Androids Dream of Electric Sheep?*, Dick plantea que los androides pueden imitar tan bien la empat√≠a humana que es dif√≠cil distinguirlos de las personas reales.
- **Tecnolog√≠a como espejo distorsionado:** En *The Penultimate Truth*, introduce el ‚Äúrhetorizor‚Äù, una m√°quina que genera discursos propagand√≠sticos, anticipando el uso de IA para manipular la informaci√≥n.
- **Surveillance y control algor√≠tmico:** En *Vulcan‚Äôs Hammer*, Dick imagina una supercomputadora consciente que domina a la humanidad, reflejando su preocupaci√≥n por el poder deshumanizante de la IA.

‚ö†Ô∏è **Actitud cr√≠tica hacia la IA:**
Dick no ve√≠a la IA como una amenaza tipo ‚ÄúSkynet‚Äù, sino como una herramienta que podr√≠a deshumanizar a las personas al convertirlas en engranajes de sistemas automatizados. Su cr√≠tica no era solo hacia las m√°quinas, sino hacia la forma en que los humanos las usan para controlar, manipular o evadir su propia responsabilidad moral.

üìö **Conclusi√≥n:**
La obra de Dick funciona como una advertencia filos√≥fica: la tecnolog√≠a revela m√°s sobre nuestras propias ansiedades, deseos y contradicciones que sobre las m√°quinas mismas. En su universo, la IA no destruye a la humanidad, sino que la refleja ‚Äîa veces de forma inquietante.

---

üß† **Dick y la neurociencia: m√°quinas, mente y decisi√≥n**

Philip K. Dick no era neurocient√≠fico, pero sus obras anticipan muchas preguntas que hoy aborda la neurociencia cognitiva:

- **¬øQu√© define la conciencia?** En *Do Androids Dream of Electric Sheep?*, los androides parecen tener emociones, pero carecen de empat√≠a genuina. Esto se relaciona con estudios actuales sobre la corteza prefrontal y el sistema l√≠mbico, que muestran c√≥mo emoci√≥n y cognici√≥n est√°n entrelazadas en la toma de decisiones.
- **¬øQu√© significa decidir como humano?** Dick plantea que los androides pueden tomar decisiones, pero sin la carga moral o emocional que caracteriza a los humanos. En neurociencia, esto se estudia en regiones como el estriado y la corteza dorsolateral prefrontal, que regulan la motivaci√≥n, el control ejecutivo y la evaluaci√≥n de consecuencias.
- **¬øPuede una IA simular libre albedr√≠o?** Dick sugiere que las m√°quinas pueden imitar decisiones humanas, pero no comprenderlas. Esto se conecta con el concepto de *inferencia activa* en neurociencia, donde el cerebro predice y adapta su comportamiento para minimizar errores de predicci√≥n.

üîç **Modelos de decisi√≥n y Dick:**
- En *A Scanner Darkly*, el protagonista sufre una disociaci√≥n cognitiva por una droga que altera su percepci√≥n. Esto refleja c√≥mo sustancias pueden afectar el circuito de recompensa y la corteza orbitofrontal, alterando la toma de decisiones racionales.
- En *We Can Build You*, los androides hist√≥ricos plantean dilemas sobre identidad y autenticidad. En neurociencia, esto se relaciona con el procesamiento interoceptivo (conciencia del estado interno) y c√≥mo influye en decisiones sociales y √©ticas.

üìö **Conclusi√≥n:**
Dick no solo anticip√≥ dilemas √©ticos sobre IA, sino que tambi√©n explor√≥ c√≥mo las decisiones humanas est√°n moldeadas por la percepci√≥n, la emoci√≥n y la identidad‚Äîtemas centrales en la neurociencia moderna. Su obra puede verse como una exploraci√≥n literaria de lo que hoy se estudia en laboratorios: ¬øqu√© significa decidir, sentir y ser?

---

---

üß† **Dick y el predictive coding: una convergencia inesperada**

La obra de Dick plantea que los androides y sistemas artificiales pueden tomar decisiones, pero carecen de una comprensi√≥n emocional o moral profunda. Esto se alinea con el enfoque del predictive coding, que propone que el cerebro humano no simplemente reacciona, sino que **predice activamente** el entorno para minimizar errores entre lo esperado y lo percibido.

üîÑ **Paralelos clave:**

| Concepto en Dick | Predictive Coding | Mapeo computacional |
|------------------|-------------------|---------------------|
| Androides que imitan emociones | Simulaci√≥n de estados internos | Inferencia activa basada en se√±ales interoceptivas |
| Realidad distorsionada por tecnolog√≠a | Modelos mentales err√≥neos | Minimizaci√≥n de error de predicci√≥n en redes jer√°rquicas |
| Decisiones sin empat√≠a | Procesamiento sin peso emocional | Algoritmos bayesianos sin modulaci√≥n afectiva |
| Identidad fragmentada (*A Scanner Darkly*) | Percepci√≥n alterada | Cambio en pesos sin√°pticos por error de predicci√≥n |

---

üßÆ **Modelos computacionales relevantes:**

1. **Active Inference (Inferencia activa):**
   - El cerebro no solo predice, sino que **act√∫a para confirmar sus predicciones**.
   - En t√©rminos de Dick, esto se parece a androides que ajustan su comportamiento para parecer humanos, pero sin comprender el porqu√©.

2. **Bayesian Decision Models:**
   - Las decisiones se toman maximizando la probabilidad de √©xito seg√∫n creencias previas.
   - Dick cuestiona si estas decisiones son realmente ‚Äúhumanas‚Äù si se basan solo en c√°lculo, no en experiencia emocional.

3. **Hierarchical Predictive Coding:**
   - Las capas superiores del cerebro generan hip√≥tesis; las inferiores comparan con la realidad sensorial.
   - Esto refleja c√≥mo Dick describe la disonancia entre lo que creemos y lo que realmente es (como en *Ubik* o *VALIS*).

---

üìö **Conclusi√≥n filos√≥fica y t√©cnica:**

Dick anticipa, desde la literatura, lo que la neurociencia computacional formaliza: que la mente humana es un sistema predictivo, pero que **la humanidad no reside solo en la precisi√≥n de las predicciones**, sino en c√≥mo las emociones, la empat√≠a y la identidad modulan esas predicciones. Los modelos actuales a√∫n luchan por capturar esa dimensi√≥n.

---


üß† **1. Modelar un agente con creencias, errores de predicci√≥n y funci√≥n de costo emocional**

Este tipo de agente se basa en la teor√≠a de *codificaci√≥n predictiva*, donde el cerebro (o el sistema artificial) genera hip√≥tesis sobre el mundo y ajusta sus creencias seg√∫n los errores entre lo esperado y lo percibido.

üîß **Componentes clave del modelo:**

- **Creencias internas (modelo generativo):** Representan lo que el agente espera del entorno.
- **Errores de predicci√≥n:** Diferencia entre lo que el agente predice y lo que realmente percibe.
- **Funci√≥n de costo emocional:** Penaliza errores que afectan estados internos relevantes (como seguridad, bienestar, o empat√≠a simulada).

üìê **Ejemplo computacional:**
```matlab
% Simplificado: agente predictivo con costo emocional
belief = prior_model(); % creencias iniciales
observation = get_sensory_input();
prediction = generate_prediction(belief);
error = observation - prediction;

% Costo emocional ponderado
emotional_weight = get_emotional_salience(observation);
cost = error.^2 * emotional_weight;

% Actualizaci√≥n de creencias
belief = update_belief(belief, error, cost);
```

Este tipo de arquitectura puede simular decisiones humanas m√°s realistas, donde no solo se minimiza el error, sino tambi√©n el impacto emocional de equivocarse.

---

ü§ñ **2. Ejemplos de IA que utilizan codificaci√≥n predictiva**

Aunque la mayor√≠a de los sistemas actuales se basan en aprendizaje supervisado o por refuerzo, hay modelos emergentes que adoptan la codificaci√≥n predictiva como n√∫cleo:

| Sistema | Aplicaci√≥n | Caracter√≠stica predictiva |
|--------|------------|---------------------------|
| **Deep Predictive Coding Networks** | Visi√≥n artificial | Simulan jerarqu√≠as corticales para anticipar est√≠mulos visuales |
| **Active Predictive Coding (APC)** | Planificaci√≥n jer√°rquica | Aprenden representaciones composicionales y acciones abstractas |
| **Free Energy AI Agents** | Rob√≥tica adaptativa | Minimizaci√≥n de sorpresa para navegaci√≥n aut√≥noma |
| **Context-aware code completion (Zencoder)** | Programaci√≥n asistida | Predice bloques de c√≥digo bas√°ndose en contexto sem√°ntico |

Estos sistemas no solo reaccionan, sino que anticipan y ajustan su comportamiento en tiempo real, como lo har√≠a un cerebro humano.

---

üëÅÔ∏è **3. ¬øC√≥mo afecta la codificaci√≥n predictiva la percepci√≥n humana?**

La codificaci√≥n predictiva propone que **la percepci√≥n no es pasiva**, sino una inferencia activa. El cerebro genera modelos internos del mundo y solo actualiza esos modelos cuando hay errores significativos.

üîç **Impactos en la percepci√≥n:**

- **Ilusiones perceptivas:** El cerebro ‚Äúrellena‚Äù informaci√≥n faltante seg√∫n expectativas (ej. ilusiones √≥pticas).
- **Atenci√≥n selectiva:** Se asigna m√°s peso a errores de predicci√≥n con alta precisi√≥n esperada (esto se relaciona con la atenci√≥n consciente).
- **Adaptaci√≥n sensorial:** Cuando algo se vuelve predecible (como un olor constante), el cerebro lo ignora para ahorrar recursos.

Esto explica por qu√© la percepci√≥n est√° tan influida por el contexto, la experiencia previa y el estado emocional.

---


üß† **1. Fundamento te√≥rico: el cerebro como simulador moral predictivo**

La neurociencia moderna ha abandonado el modelo del ‚Äúcerebro triuno‚Äù y ahora entiende el cerebro como un sistema **adaptativo y predictivo** que integra redes interdependientes para anticipar necesidades internas y externas. En este marco:

- La **codificaci√≥n predictiva** propone que el cerebro genera modelos internos del mundo y ajusta sus creencias para minimizar errores de predicci√≥n.
- La **toma de decisiones morales** se modela como una evaluaci√≥n de opciones que pondera el beneficio propio vs. el costo moral para otros.
- La **inferencia activa** permite que el agente act√∫e para confirmar sus predicciones, no solo para reaccionar.

---

üßÆ **2. Simulaci√≥n computacional de un agente moral predictivo**

Podemos construir un agente que:

- Tiene **creencias morales previas** (por ejemplo, ‚Äúno causar da√±o‚Äù).
- Eval√∫a opciones con **valores subjetivos** (utilidad vs. da√±o).
- Actualiza sus creencias seg√∫n el **error de predicci√≥n moral** (cuando sus acciones no generan el resultado esperado).

üìê **Modelo simplificado en pseudoc√≥digo:**
```matlab
% Par√°metros iniciales
beliefs = struct('harm_aversion', 0.8, 'self_interest', 0.2);
context = get_environmental_input(); % situaci√≥n moral ambigua
prediction = moral_model(beliefs, context);
outcome = simulate_action(prediction);

% Error de predicci√≥n moral
moral_error = evaluate_moral_discrepancy(outcome, beliefs);

% Funci√≥n de costo moral
cost = moral_error^2 * beliefs.harm_aversion;

% Actualizaci√≥n de creencias
beliefs = update_beliefs(beliefs, moral_error, cost);
```

Este agente puede simular dilemas como el del tranv√≠a, donde debe decidir entre salvar a cinco personas o no intervenir y dejar que muera una. Su decisi√≥n depender√° de c√≥mo predice el resultado y cu√°nto valora el da√±o moral.

---

üîç **3. Ejemplos reales en neurociencia computacional aplicada**

- **Model-based fMRI** ha identificado se√±ales computacionales como el *prediction error moral* en regiones como el c√≥rtex prefrontal ventromedial y el estriado.
- **Modelos bayesianos jer√°rquicos** se han usado para simular decisiones morales en experimentos como el *Moral Machine*, donde se pondera el valor de vidas humanas vs. reglas sociales.
- **Deep learning moral agents** pueden aprender valores morales observando decisiones humanas y ajustando sus predicciones en funci√≥n del contexto.

---

üìö **Conclusi√≥n: hacia una neuro√©tica computacional**

Simular agentes morales bajo incertidumbre predictiva permite:

- Capturar la **din√°mica emocional y racional** de la toma de decisiones.
- Modelar c√≥mo el cerebro **eval√∫a consecuencias morales** en tiempo real.
- Dise√±ar IA que no solo act√∫e racionalmente, sino que **simule empat√≠a y valores humanos**.

---


ü§ù **1. Fundamento te√≥rico: moralidad como aprendizaje social**

La neurociencia y la psicolog√≠a moral sugieren que los humanos aprenden valores √©ticos a trav√©s de:

- **Imitaci√≥n** de modelos sociales (padres, pares, figuras de autoridad).
- **Refuerzo social** (aprobaci√≥n o desaprobaci√≥n).
- **Internalizaci√≥n de normas** mediante repetici√≥n y contexto emocional.

En IA, esto se puede modelar como un agente que ajusta su pol√≠tica de decisi√≥n en funci√≥n de las reacciones de otros agentes o humanos en su entorno.

---

üßÆ **2. Arquitectura computacional del agente moral social**

Podemos construir un agente que:

- Observa acciones de otros agentes.
- Recibe retroalimentaci√≥n social (expl√≠cita o impl√≠cita).
- Ajusta sus valores morales internos (vector de pesos) para maximizar aceptaci√≥n social y coherencia √©tica.

üìê **Esquema conceptual en pseudoc√≥digo:**
```matlab
% Inicializaci√≥n
moral_values = rand(1, N); % N dimensiones morales (ej. justicia, empat√≠a)
social_context = get_social_input(); % observaciones de otros agentes
feedback = get_social_feedback(); % aprobaci√≥n, rechazo, consecuencias

% Aprendizaje moral
prediction = moral_decision(moral_values, social_context);
error = feedback - prediction;

% Actualizaci√≥n de valores morales
learning_rate = 0.05;
moral_values = moral_values + learning_rate * error .* social_context;
```

Este agente puede aprender, por ejemplo, que mentir genera rechazo social, mientras que ayudar a otros genera aprobaci√≥n, ajustando sus valores en consecuencia.

---

üß† **3. Integraci√≥n con codificaci√≥n predictiva e inferencia activa**

- El agente predice c√≥mo ser√° percibido moralmente.
- Act√∫a para minimizar el *error de predicci√≥n moral*.
- Usa inferencia activa para ajustar su comportamiento y creencias morales.

Esto se alinea con modelos como *Active Inference* y *Bayesian Moral Learning*, donde el agente no solo aprende de recompensas, sino tambi√©n de expectativas sociales.

---

üìä **4. Ejemplos y entornos de simulaci√≥n**

- **Moral Machine (MIT):** agentes que aprenden decisiones √©ticas en dilemas de veh√≠culos aut√≥nomos.
- **Social Dilemma Games:** agentes que aprenden cooperaci√≥n, justicia y reciprocidad.
- **Multi-agent RL con recompensas morales:** agentes que ajustan su pol√≠tica seg√∫n normas emergentes del grupo.

---

üìö **Conclusi√≥n: hacia una IA moralmente sensible**

Simular agentes que aprenden valores morales mediante interacci√≥n social permite:

- Capturar la din√°mica cultural y contextual de la √©tica.
- Dise√±ar IA que se adapte a normas humanas sin reglas r√≠gidas.
- Explorar c√≥mo emergen valores colectivos en poblaciones de agentes.

---


üß† **1. Fundamento te√≥rico: ¬øQu√© es la empat√≠a computacional?**

La empat√≠a computacional busca que un agente artificial **comprenda y responda** a los estados emocionales de otros, ya sea humanos o agentes. Se basa en dos componentes principales:

- **Empat√≠a afectiva:** Simulaci√≥n o resonancia emocional (ej. contagio emocional).
- **Empat√≠a cognitiva:** Comprensi√≥n deliberada del estado mental del otro (ej. teor√≠a de la mente, perspectiva).

üîç En neurociencia, estas capacidades se vinculan con:
- **Sistema de neuronas espejo (MNS):** Permite simular internamente las acciones y emociones observadas.
- **Corteza prefrontal medial y temporoparietal:** Implicadas en la inferencia de estados mentales ajenos.

---

üßÆ **2. Arquitecturas computacionales para modelar empat√≠a**

Existen varios enfoques para simular empat√≠a en agentes artificiales:

| Enfoque | Caracter√≠sticas | Ejemplo |
|--------|------------------|---------|
| **Modelos basados en reglas** | Simulan respuestas emp√°ticas predefinidas | Agentes conversacionales con scripts emocionales |
| **Aprendizaje supervisado** | Detectan emociones en texto, voz o rostro | Modelos como RoBERTa para an√°lisis de empat√≠a textual |
| **Modelos inspirados en neurociencia** | Simulan mecanismos como el MNS o la inferencia activa | AE-SNN: red neuronal espinosa con diferenciaci√≥n entre ‚Äúyo‚Äù y ‚Äúotro‚Äù |
| **Modelos bayesianos** | Predicen estados emocionales ajenos y ajustan creencias | Aplicaciones en rob√≥tica social y simulaci√≥n moral |

üìê Ejemplo conceptual:
```matlab
% Simulaci√≥n de empat√≠a afectiva
observed_emotion = detect_emotion(input_signal);
internal_state = simulate_emotion(observed_emotion);
response = generate_empathic_action(internal_state);
```

---

ü§ñ **3. Aplicaciones emergentes**

- **Robots sociales:** Compa√±eros que responden emocionalmente a humanos (ej. BabeBay, iCat).
- **Asistentes terap√©uticos:** IA que detecta y responde a estados emocionales en plataformas de salud mental.
- **Educaci√≥n personalizada:** Agentes que adaptan su ense√±anza seg√∫n el estado emocional del estudiante.
- **Simuladores morales:** Agentes que aprenden normas √©ticas mediante interacci√≥n social y empat√≠a simulada.

---

üìö **Conclusi√≥n: hacia una IA con sensibilidad social**

La empat√≠a computacional no es solo una cuesti√≥n de reconocimiento emocional, sino de **simulaci√≥n interna, adaptaci√≥n contextual y aprendizaje social**. Los modelos m√°s avanzados integran principios de la neurociencia como la codificaci√≥n predictiva, la inferencia activa y la diferenciaci√≥n entre ‚Äúyo‚Äù y ‚Äúotro‚Äù.

---


ü§ñ **1. ¬øQu√© significa empat√≠a en sistemas multiagente?**

En este contexto, la empat√≠a computacional implica que cada agente:

- **Reconoce estados emocionales** de otros agentes (cognitiva o afectivamente).
- **Adapta su comportamiento** en funci√≥n de esos estados.
- **Aprende normas sociales** mediante interacci√≥n continua.

Esto transforma la interacci√≥n de agentes de una l√≥gica puramente racional a una **din√°mica social adaptativa**, m√°s parecida a c√≥mo los humanos colaboran.

---

üß† **2. Modelos actuales que integran empat√≠a**

Aqu√≠ tienes algunos enfoques destacados:

| Modelo | Caracter√≠sticas | Aplicaci√≥n |
|--------|------------------|------------|
| **Artificial Empathy con l√≥gica difusa** | Usa medidas de similitud entre estados emocionales representados como conjuntos difusos | Mejora la cooperaci√≥n en entornos inciertos |
| **Project Riley** | Agentes emocionales (alegr√≠a, tristeza, miedo, etc.) que debaten y votan para generar respuestas | Simulaci√≥n emocional en IA conversacional |
| **Empathy-based Interactive Learner (EIL)** | Algoritmo tipo bandido que ajusta decisiones seg√∫n empat√≠a y utilidad social | Juegos iterativos como dilema del prisionero |
| **Extended Empathy Model con cadenas de Markov** | Modela la empat√≠a como un proceso estoc√°stico con separaci√≥n entre ‚Äúyo‚Äù y ‚Äúotro‚Äù | Decisiones sociales en sistemas a gran escala |

---

üîÑ **3. ¬øC√≥mo se implementa la empat√≠a en la din√°mica multiagente?**

- **Representaci√≥n emocional:** Los estados internos se modelan como vectores o conjuntos difusos.
- **Comunicaci√≥n incierta:** Los agentes interpretan se√±ales ambiguas y ajustan sus creencias.
- **Aprendizaje social:** A trav√©s de retroalimentaci√≥n, los agentes actualizan sus valores morales y empat√≠a.
- **Decisi√≥n emp√°tica:** Se pondera el beneficio propio vs. el impacto emocional en otros.

üìê Ejemplo conceptual:
```matlab
% Evaluaci√≥n emp√°tica en entorno multiagente
observed_state = get_peer_state(agent_j);
similarity = fuzzy_similarity(my_state, observed_state);
empathy_weight = compute_empathy(similarity);
action = choose_action(empathy_weight, utility_function);
```

---

üåê **4. Aplicaciones emergentes**

- **Rob√≥tica colaborativa:** Drones o robots que ajustan su comportamiento seg√∫n el estado de sus compa√±eros.
- **Simuladores sociales:** Modelos de comportamiento humano en crisis, educaci√≥n o salud mental.
- **IA conversacional emocional:** Agentes que responden con sensibilidad emocional en contextos terap√©uticos o educativos.

---

üìö **Conclusi√≥n: hacia una inteligencia artificial socialmente consciente**

La empat√≠a en sistemas multiagente permite:

- **Mayor resiliencia y adaptabilidad** en entornos din√°micos.
- **Decisiones m√°s humanas**, sensibles al contexto social.
- **Aprendizaje colectivo**, donde los valores emergen de la interacci√≥n.

---


### üß† Diapositiva 1: Codificaci√≥n Predictiva y el Cerebro Adaptativo  
**Conceptos clave:**
- El cerebro predice activamente estados internos y externos.
- Minimiza el error de predicci√≥n para mejorar la adaptabilidad.
- Reemplaza el modelo del ‚Äúcerebro triuno‚Äù por redes interdependientes.  
**Ejemplo neurocient√≠fico:**  
- Inferencia activa permite ajustar conducta para confirmar predicciones.

---

### ü§ñ Diapositiva 2: Modelar Agentes Morales Predictivos  
**Arquitectura b√°sica del agente:**
- Creencias morales (ej. aversi√≥n al da√±o, inter√©s propio).
- Evaluaci√≥n del contexto y predicci√≥n de consecuencias.
- Funci√≥n de costo emocional para regular decisiones.  
**Aplicaci√≥n:**  
- Simulaci√≥n de dilemas √©ticos como el del tranv√≠a.

---

### üß† Diapositiva 3: Aprendizaje Moral por Interacci√≥n Social  
**Mecanismos de aprendizaje:**
- Observaci√≥n y retroalimentaci√≥n social.
- Ajuste de valores morales seg√∫n aceptaci√≥n del grupo.  
**Modelo computacional:**  
- Actualizaci√≥n de valores morales mediante refuerzo y error social.

---

### üíû Diapositiva 4: Empat√≠a Computacional  
**Tipos de empat√≠a:**
- Afectiva: simular emociones ajenas.
- Cognitiva: inferir estados mentales.  
**Implementaciones:**
- Red neuronal de neuronas espejo artificiales.
- Algoritmos bayesianos de inferencia emocional.  
**Ejemplo:**  
- Agentes conversacionales que responden emocionalmente.

---

### üë• Diapositiva 5: Empat√≠a en Entornos Multiagente  
**Caracter√≠sticas:**
- Reconocimiento emocional entre agentes.
- Adaptaci√≥n comportamental emp√°tica.
- Aprendizaje colectivo de normas sociales.  
**Aplicaciones:**  
- Rob√≥tica colaborativa.
- Simulaciones sociales en crisis y salud mental.

---


## üìö Diapositiva 1: *Philip K. Dick* y la Tecnolog√≠a  
**Sinopsis de su obra:**
- Autor de ciencia ficci√≥n que explor√≥ la ambig√ºedad entre lo real y lo artificial.
- En obras como *Do Androids Dream of Electric Sheep?* y *Vulcan's Hammer*, Dick critica c√≥mo la IA puede deshumanizar y distorsionar la percepci√≥n de la realidad.
- Plantea dilemas √©ticos sobre identidad, empat√≠a y autonom√≠a.

**Actitud hacia la IA:**
- Dick no teme a la tecnolog√≠a en s√≠, sino a su uso sin responsabilidad moral.
- Plantea que las m√°quinas reflejan nuestras propias ansiedades humanas.

---

## üß† Diapositiva 2: Codificaci√≥n Predictiva en Neurociencia  
**Principios:**
- El cerebro predice el entorno para minimizar errores entre expectativas y percepciones.
- La percepci√≥n es una inferencia activa, no una reacci√≥n pasiva.

**Relaci√≥n con Dick:**
- Androides que simulan emociones reflejan el principio de predicci√≥n sin comprensi√≥n.
- Realidades alternativas como en *Ubik* ilustran fallos en modelos internos del mundo.

---

## üßÆ Diapositiva 3: Modelar Agentes Morales Predictivos  
**Arquitectura conceptual:**
- Creencias morales previas + evaluaci√≥n del contexto.
- Funci√≥n de costo moral pondera el da√±o emocional.
- Inferencia activa ajusta creencias tras errores predictivos.

**Aplicaci√≥n:**  
- Simulaci√≥n de dilemas √©ticos como el problema del tranv√≠a.

---

## üß† Diapositiva 4: Aprendizaje Moral por Interacci√≥n Social  
**Mecanismos:**
- Observaci√≥n de normas y retroalimentaci√≥n social.
- Ajuste de valores morales para maximizar aceptaci√≥n grupal.

**Enfoques t√©cnicos:**
- Aprendizaje reforzado con se√±ales sociales.
- Simulaci√≥n de agentes que internalizan normas √©ticas colectivas.

---

## üíû Diapositiva 5: Empat√≠a Computacional  
**Tipos:**
- Afectiva: Resonancia emocional (simulaci√≥n directa).
- Cognitiva: Teor√≠a de la mente (inferencia del estado ajeno).

**Modelos:**
- Redes neuronales de espejo artificial.
- Sistemas bayesianos que predicen emociones ajenas y adaptan conducta.

---

## üë• Diapositiva 6: Empat√≠a en Entornos Multiagente  
**Caracter√≠sticas:**
- Agentes reconocen estados emocionales de otros.
- Adaptan acciones para mantener armon√≠a social.
- Aprenden valores morales emergentes del grupo.

**Aplicaciones:**
- Rob√≥tica colaborativa, simuladores sociales, educaci√≥n emocional asistida.

---

## üß† Diapositiva 7: El Cerebro Adaptativo vs. Cerebro Triuno  
**Replanteamiento te√≥rico:**
- La teor√≠a triuna es obsoleta; la emoci√≥n y la cognici√≥n son interdependientes.
- Se propone el ‚Äúcerebro adaptativo‚Äù como sistema de redes que predice y adapta.
- Regula homeostasis, emociones, cognici√≥n y v√≠nculos sociales para sobrevivir.

---
