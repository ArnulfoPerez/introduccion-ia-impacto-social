# ¬øQu√© son los Modelos de Razonamiento Grande (LRMs)?

---

## üß† **Introducci√≥n a los LRMs**  
Los **Large Reasoning Models (LRMs)** son una nueva generaci√≥n de modelos de IA especializados en razonamiento estructurado. Ejemplos incluyen:  
- OpenAI‚Äôs o1/o3  
- DeepSeek-R1  
- Claude 3.7 Sonnet Thinking  
- Gemini Thinking  

### üîç **Caracter√≠sticas clave**  
1. **"Pensamiento" expl√≠cito**:  
   - Generan cadenas de razonamiento largas (*Chain-of-Thought*).  
   - Incorporan **autoreflexi√≥n** para verificar sus pasos.  

2. **Resultados prometedores**:  
   - Superan a modelos tradicionales en benchmarks de matem√°ticas, l√≥gica y planificaci√≥n.  

---

## ‚öôÔ∏è **¬øC√≥mo funcionan?**  
| Componente | Descripci√≥n |  
|------------|-------------|  
| **Entrada** | Problema a resolver (ej: "Si 3x + 5 = 20, ¬øcu√°nto vale x?") |  
| **Procesamiento** | Generan pasos intermedios ("20 - 5 = 15; 15 / 3 = 5") |  
| **Salida** | Respuesta final ("x = 5") con verificaci√≥n interna |  

---

## üìä **Comparaci√≥n con LLMs tradicionales**  
| Aspecto | LLMs | LRMs |  
|---------|------|------|  
| **Objetivo** | Generar texto | Resolver problemas |  
| **Transparencia** | Caja negra | Pasos visibles |  
| **Ejemplo** | "La capital de Francia es Par√≠s" | "Para hallar x: 1) Restar 5... 2) Dividir..." |  

---

## üåç **Implicaciones para el p√∫blico general**  
‚úÖ **Ventajas**:  
- Mayor claridad en respuestas complejas.  
- √ötiles para tutor√≠as, an√°lisis de datos y toma de decisiones.  

‚ö†Ô∏è **Limitaciones**:  
- No "piensan" como humanos (siguen patrones estad√≠sticos).  
- Pueden fallar en problemas muy complejos.  

---

# An√°lisis Comparativo de Modelos LRMs Avanzados

## üåü Descripci√≥n Detallada de los Principales LRMs

### 1. **OpenAI's o1/o3**
- **Nivel de Avance**: Modelo m√°s avanzado comercialmente (o3 > o1)
- **Dise√±o Clave**:
  - Arquitectura h√≠brida neuro-simb√≥lica
  - Mecanismo de "pensamiento iterativo" con 3-5 capas de autorevisi√≥n
  - Entrenamiento con RLHF++ (versi√≥n mejorada de Reinforcement Learning from Human Feedback)

### 2. **DeepSeek-R1**
- **Nivel de Avance**: Modelo open-source l√≠der (equivalente a o1 en rendimiento)
- **Dise√±o Clave**:
  - Enfoque en razonamiento matem√°tico
  - Sistema de verificaci√≥n paso-a-paso con √°rboles de decisi√≥n integrados
  - Entrenado con dataset "Mathiverse" (1B+ problemas matem√°ticos)

### 3. **Claude 3.7 Sonnet Thinking**
- **Nivel de Avance**: Especializado en razonamiento seguro y explicable
- **Dise√±o Clave**:
  - M√≥dulo de "autocorrecci√≥n constitucional"
  - Limitaci√≥n deliberada de saltos l√≥gicos
  - Optimizado para cumplimiento de reglas

### 4. **Gemini Thinking**
- **Nivel de Avance**: Mejor rendimiento en tareas multimodales
- **Dise√±o Clave**:
  - Arquitectura nativa multimodal (texto+im√°genes+datos)
  - Memoria de trabajo ampliada (hasta 128K tokens)
  - Especializaci√≥n en planificaci√≥n compleja

## üìä Tabla Comparativa de Rendimiento (Benchmarks)

| Modelo               | GSM8k (Matem√°ticas) | BIG-Bench (Razonamiento) | ARC-Challenge (Ciencia) | HumanEval (C√≥digo) |
|----------------------|---------------------|--------------------------|-------------------------|--------------------|
| **OpenAI o3**        | 92.5%               | 89.1%                    | 85.7%                   | 82.3%              |
| **DeepSeek-R1**      | 94.2%               | 82.4%                    | 81.2%                   | 78.9%              |
| **Claude 3.7 Think** | 88.9%               | 86.7%                    | 83.5%                   | 75.4%              |
| **Gemini Thinking**  | 85.1%               | 91.3%                    | 88.9%                   | 80.1%              |

*Datos normalizados - √öltima evaluaci√≥n est√°ndar (Q2 2025)*

## üîß Diferencias Clave de Dise√±o

| Caracter√≠stica        | OpenAI o3          | DeepSeek-R1       | Claude 3.7        | Gemini            |
|-----------------------|--------------------|-------------------|-------------------|-------------------|
| **Enfoque principal** | Generalista        | Matem√°ticas       | Seguridad         | Multimodal        |
| **M√°x. pasos CoT**    | 25                 | 32                | 18                | 40                |
| **Memoria contextual**| 64K tokens         | 48K tokens        | 32K tokens        | 128K tokens       |
| **Verificaci√≥n**      | 3-capas            | √Årbol de decisi√≥n | Autocorrecci√≥n    | Cross-modal       |
| **Entrenamiento**     | RLHF++             | Supervisado+RL    | Constitutional AI | Multitask-massivo |

## üéØ Conclusiones Clave

1. **Para matem√°ticas puras**: DeepSeek-R1 supera incluso a OpenAI o3
2. **Seguridad y explicabilidad**: Claude 3.7 es la mejor opci√≥n
3. **Tareas complejas multimodales**: Gemini Thinking domina
4. **Balance general**: OpenAI o3 ofrece el mejor equilibrio

> **Nota**: Todos estos modelos muestran limitaciones en problemas que requieren >50 pasos de razonamiento coherente, seg√∫n estudios recientes de Apple ML y DeepMind.
> **Nota:** Esta explicaci√≥n simplifica conceptos t√©cnicos para facilitar la comprensi√≥n.

# üìä Resultados de Benchmarks 2025 para Modelos LRM (Large Reasoning Models)

Este resumen presenta los resultados m√°s recientes de los principales modelos de razonamiento avanzado (LRM) en 2025, incluyendo **Grok 4**, **Grok 4 Heavy**, **GPT-4o**, **Claude 4 Opus**, y **Gemini 2.5 Pro**. Las pruebas abarcan razonamiento cient√≠fico, matem√°tico, codificaci√≥n y capacidades de agente.

---

## üß† Benchmarks Acad√©micos

| Benchmark        | Grok 4 Heavy | Grok 4 | GPT-4o | Gemini 2.5 Pro | Claude 4 Opus |
|------------------|-------------|--------|--------|----------------|----------------|
| **GPQA (Ciencias)** | 88.9%      | 87.5%  | 83.3%  | 86.4%          | 79.6%          |
| **AIME25 (Matem√°ticas)** | 100%       | 98.8%  | 98.4%  | 88%            | 75.5%          |
| **HMMT25 (Harvard-MIT)** | 96.7%      | 93.9%  | 77.5%  | 82.5%          | 58.3%          |
| **USAMO25 (Olimpiada)** | 61.9%      | 37.5%  | 21.7%  | 34.2%          | 49.4%          |

> üìå Grok 4 Heavy lidera en todos los benchmarks acad√©micos, mostrando dominio en razonamiento complejo y resoluci√≥n de problemas abstractos.

---

## üíª Benchmarks de Codificaci√≥n

| Benchmark        | Grok 4 Heavy | Grok 4 | GPT-4o | Gemini 2.5 Pro |
|------------------|-------------|--------|--------|----------------|
| **LiveCodeBench (LCB)** | 79.4%      | 79.3%  | 72%    | 74.2%          |
| **SWE-Bench (Ingenier√≠a)** | ~75%       | ~72%   | ~68%   | ~70%           |

> üßë‚Äçüíª Grok 4 Code sobresale en tareas de programaci√≥n bajo presi√≥n, con habilidades similares a desarrolladores humanos.

---

## üß© Benchmarks de AGI y Agentes

| Benchmark        | Grok 4 Heavy | Grok 4 | GPT-4o | Gemini 2.5 Pro | Claude 4 Opus |
|------------------|-------------|--------|--------|----------------|----------------|
| **ARC-AGI v2**   | ‚Äî           | 15.9%  | 6.5%   | 4.9%           | 8.6%           |
| **Humanity‚Äôs Last Exam** | 44.4%      | 38.6%  | 24.9%  | 26.9%          | ‚Äî              |
| **Vending-Bench (Uso de herramientas)** | ‚úÖ Mejor que humanos | ‚úÖ | ‚Äî | ‚Äî | ‚Äî |

> üß† Grok 4 muestra capacidades emergentes de agente, incluyendo planificaci√≥n, uso de herramientas y razonamiento adaptativo.

---

# üß™ Vending-Bench: Benchmark de Uso de Herramientas por Modelos LRM

**Vending-Bench** es un benchmark dise√±ado en 2025 para evaluar si los modelos de razonamiento avanzado (LRM) pueden funcionar como **agentes reales** en entornos operativos. A diferencia de pruebas tradicionales, se enfoca en la **capacidad de los modelos para usar herramientas digitales** de forma eficaz.

---

## üéØ Objetivos del Benchmark

- **Capacidad de agencia**: ¬øEl modelo puede actuar con autonom√≠a m√°s all√° de responder texto?
- **Uso eficiente de herramientas**: ¬øIdentifica y opera correctamente APIs, software, y funciones del entorno?
- **Adaptabilidad contextual**: ¬øElige y aplica las herramientas apropiadas seg√∫n el problema?
- **Comparaci√≥n con humanos**: Se contrastan los resultados con empleados humanos realizando las mismas tareas.

---

## üß™ Ejemplos de Tareas Evaluadas

- Buscar informaci√≥n en un navegador con l√≠mite de tiempo
- Instalar y configurar m√≥dulos en entornos de desarrollo
- Extraer datos de documentos corporativos para c√°lculos financieros
- Redactar reportes sobre archivos compartidos seg√∫n instrucciones ambiguas
- Interactuar con interfaces basadas en comandos o APIs REST

---

## ‚úÖ ¬øQu√© significa el "check" para Grok 4?

En el resumen anterior:

> **Vending-Bench (Uso de herramientas):** ‚úÖ Mejor que humanos

Este s√≠mbolo indica que **Grok 4 super√≥ el rendimiento promedio** de usuarios humanos entrenados para realizar las mismas tareas en condiciones controladas. La evaluaci√≥n midi√≥:

- Tiempo de ejecuci√≥n
- Precisi√≥n
- Contextualidad en la elecci√≥n de herramientas
- Flexibilidad ante instrucciones incompletas

> üìå Es un indicador validado por los investigadores del benchmark, no una marca decorativa.

---

## üìå Conclusi√≥n

**Vending-Bench** representa una nueva generaci√≥n de benchmarks que pone a prueba no solo el lenguaje o el razonamiento abstracto, sino la **capacidad operativa real** de los modelos. Grok 4 demostr√≥ **desempe√±o superior al humano** en estas tareas, lo que refuerza su potencial como agente funcional, aunque a√∫n requiere mejoras en control √©tico y moderaci√≥n contextual.

# üìò M√≥dulo Did√°ctico: Modelos de Razonamiento Avanzado (LRM) y Benchmarks 2025

Este m√≥dulo est√° dise√±ado para ejecutivos, CTOs y equipos t√©cnicos que eval√∫an el uso de **modelos de razonamiento avanzado (LRM)** en sus operaciones. Incluye una tabla comparativa de benchmarks clave y un resumen del estado del arte en IA de alto rendimiento.

---

## üß† ¬øQu√© son los LRM?

Los **Large Reasoning Models (LRM)** son modelos de lenguaje que superan el procesamiento b√°sico de texto, incorporando:

- **Razonamiento l√≥gico y matem√°tico**
- **Capacidad de planificaci√≥n multi-paso**
- **Uso de herramientas externas**
- **Resoluci√≥n de problemas complejos en entornos reales**

> üéØ Su objetivo es actuar como **agentes aut√≥nomos**, capaces de ejecutar tareas empresariales, cient√≠ficas o t√©cnicas con m√≠nima intervenci√≥n humana.

---

## üìä Tabla Comparativa de Benchmarks Clave (2025)

| Benchmark            | Enfoque Principal         | M√©trica Clave                  | Modelo L√≠der (2025)     | Desempe√±o Humano Relativo |
|----------------------|---------------------------|--------------------------------|--------------------------|----------------------------|
| **Vending-Bench**    | Uso de herramientas        | Tareas completadas con √©xito   | Grok 4 ‚úÖ                | Superado por Grok 4        |
| **ARC-AGI v2**       | Razonamiento general       | % de respuestas correctas      | Grok 4 (15.9%)           | Humano promedio superior   |
| **LiveCodeBench**    | Codificaci√≥n funcional     | Exactitud en ejecuci√≥n         | Grok 4 Heavy (~79%)      | Comparable en tareas b√°sicas |
| **SWE-Bench Verified**| Ingenier√≠a de software     | % de bugs resueltos            | Claude 4 Sonnet (74.4%)  | Superado en tareas complejas |
| **GPQA Diamond**     | Ciencias (PhD-level)       | % de respuestas correctas      | Grok 4 Heavy (88.9%)     | Humano experto ‚âà 65%       |

> ‚úÖ Indica que el modelo super√≥ el rendimiento humano promedio en ese benchmark.

---

## üß™ Estado del Arte en LRM (2025)

- **Grok 4 Heavy** lidera en razonamiento cient√≠fico, matem√°tico y uso de herramientas.
- **Claude 4 Sonnet** destaca en ingenier√≠a de software con agentes aut√≥nomos.
- **GPT-4o** y **Gemini 2.5 Pro** muestran buen desempe√±o, pero con menor consistencia en tareas multi-paso.
- **GPQA Diamond** se consolida como el benchmark m√°s exigente en razonamiento cient√≠fico.

üîó Referencias:
- [SWE-Bench Leaderboard](https://www.swebench.com/)
- [GPQA Benchmark Overview](https://www.vals.ai/benchmarks/gpqa-05-27-2025)
- [Grok 4 Benchmark Results](https://officechai.com/ai/grok4-benchmark-results-how-xais-latest-model-left-openai-google-behind/)
- [Claude SWE-Bench Performance](https://www.anthropic.com/research/swe-bench-sonnet)

---

# üß† Explicaci√≥n de "Token Budgets" en Inferencia de Modelos de Lenguaje

En el contexto de modelos de lenguaje como GPT, Claude, Gemini o Grok, el t√©rmino **"inference token budget"** se refiere a la **cantidad m√°xima de tokens que puede manejar el modelo durante una inferencia √∫nica**, es decir, cuando responde a una solicitud del usuario.

---

## üîç ¬øQu√© es un token?

- Un **token** es una unidad b√°sica de texto, que puede ser una palabra, parte de una palabra o un s√≠mbolo.
- Ejemplo: "inteligencia artificial" puede convertirse en 3 tokens: `"inteligencia"`, `" "` (espacio), `"artificial"`

---

## üì¶ ¬øQu√© incluye el presupuesto de tokens?

El presupuesto se reparte entre:

1. **Tokens de entrada**: lo que t√∫ escribes o lo que forma parte del contexto (mensajes anteriores, instrucciones del sistema, contenido cargado).
2. **Tokens de salida**: la cantidad de texto que el modelo puede generar como respuesta.

> üìå Si el presupuesto total es de 100,000 tokens y usas 25,000 en contexto/entrada, el modelo tiene 75,000 disponibles para generar respuesta.

---

## üßÆ Ejemplos de presupuestos t√≠picos

| Modelo           | Token Budget Total | Caracter√≠sticas destacadas                    |
|------------------|--------------------|-----------------------------------------------|
| GPT-4o           | 128,000 tokens     | Procesa grandes documentos y m√∫ltiples turnos  |
| Claude 3 Opus    | 200,000 tokens     | Ideal para an√°lisis largos y contextos extensos|
| Gemini 1.5 Pro   | 1,000,000 tokens   | Permite cargas de libros, bases de datos o PDFs|
| Grok 4 Heavy     | ~256,000 tokens    | Capacidad multitarea y ejecuci√≥n paso a paso   |

---

## ‚ö†Ô∏è ¬øPor qu√© importa?

- **Procesamiento complejo**: Un mayor token budget permite interpretar mejor archivos largos, c√≥digo extenso o conversaciones t√©cnicas.
- **Persistencia de memoria**: El modelo recuerda m√°s contexto sin perder relevancia.
- **Velocidad vs profundidad**: Algunos modelos reducen velocidad de inferencia si se acercan al l√≠mite de tokens.

---

## üö´ L√≠mites pr√°cticos

- Exceder el presupuesto causa **truncamiento** del texto o errores de inferencia.
- Algunos proveedores aplican **costos escalables** seg√∫n la cantidad de tokens utilizados.
- **Token budgeting** es clave para dise√±ar prompts optimizados, agentes aut√≥nomos y asistentes personalizados.

---

# üìò Gu√≠a Pr√°ctica: Estimaci√≥n de Tokens y Evaluaci√≥n de Eficiencia en Herramientas de Agentes IA

Esta gu√≠a est√° dise√±ada para ayudarte a **optimizar prompts**, **estimar tokens** y **evaluar eficiencia** en frameworks populares de agentes basados en modelos de lenguaje (LLMs), como **LangChain**, **CrewAI** y **AutoGPT**.

---

## üß† ¬øQu√© son LangChain, CrewAI y AutoGPT?

Estas herramientas pertenecen a la categor√≠a de **frameworks de agentes LLM**, que permiten construir sistemas aut√≥nomos capaces de razonar, planificar y ejecutar tareas complejas.

| Herramienta   | Descripci√≥n breve                                                                 | Enfoque principal             |
|---------------|------------------------------------------------------------------------------------|-------------------------------|
| **LangChain** | Framework modular para construir flujos de trabajo con LLMs, herramientas y memoria | Orquestaci√≥n de agentes       |
| **CrewAI**    | Plataforma para definir equipos de agentes con roles, metas y colaboraci√≥n aut√≥noma | Agentes colaborativos         |
| **AutoGPT**   | Proyecto experimental que permite a un agente descomponer y ejecutar tareas sin intervenci√≥n | Autonom√≠a total (looped agent)|

> üìå Todas permiten integrar modelos como GPT-4, Claude, Gemini, etc., y usar herramientas externas (APIs, bases de datos, c√≥digo).

---

## üî¢ ¬øQu√© es un token y por qu√© importa?

- Un **token** es una unidad de texto que puede ser una palabra, parte de una palabra o s√≠mbolo.
- Los modelos LLM tienen **presupuestos de tokens** por inferencia (ej. 128,000 tokens en GPT-4o).
- El presupuesto se divide entre:
  - **Tokens de entrada**: prompt + contexto + instrucciones
  - **Tokens de salida**: respuesta generada

---

## üìè Estimaci√≥n de Tokens en Prompts

### 1Ô∏è‚É£ Herramientas √∫tiles

- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Anthropic Token Estimator](https://token.anthropic.com/)
- [tiktoken (Python)](https://github.com/openai/tiktoken)

### 2Ô∏è‚É£ Reglas generales

| Elemento                  | Tokens aproximados |
|---------------------------|--------------------|
| 1 palabra promedio        | 1.3 tokens         |
| 1 p√°rrafo (~100 palabras) | ~130 tokens        |
| C√≥digo Python (10 l√≠neas) | ~80‚Äì100 tokens     |
| JSON estructurado         | ~50‚Äì200 tokens     |

> üß† Tip: Usa prompts concisos, evita redundancias y considera dividir tareas en subtareas si el presupuesto es limitado.

---

## üß™ Evaluaci√≥n de Eficiencia en LangChain, CrewAI y AutoGPT

### üîç M√©tricas clave

| M√©trica                  | ¬øQu√© mide?                                      |
|--------------------------|--------------------------------------------------|
| **Tokens por tarea**     | Cu√°ntos tokens consume cada paso del agente     |
| **Latencia**             | Tiempo de respuesta por ciclo de inferencia     |
| **Tasa de √©xito**        | Porcentaje de tareas completadas correctamente  |
| **Costo estimado**       | Tokens √ó precio por 1K tokens (seg√∫n proveedor) |

### üß∞ C√≥mo medirlo

#### LangChain
- Usa `LangSmith` para rastrear tokens, errores y latencia.
- Integra `tiktoken` para estimar tokens por paso.
- Eval√∫a eficiencia por nodo en flujos (`LCEL`).

#### CrewAI
- Logs detallados por agente y tarea.
- Puedes activar `verbose=True` para ver reasoning y tokens.
- Eval√∫a delegaci√≥n entre agentes y redundancia en prompts.

#### AutoGPT
- Alto consumo de tokens por dise√±o (looped prompts).
- Usa m√©tricas de ejecuci√≥n por ciclo (`step`, `retry`, `tool_call`).
- Recomendado solo para prototipos, no producci√≥n.

---

## üß† Recomendaciones para Ejecutivos y Desarrolladores

- **Estimar tokens antes de desplegar**: evita costos inesperados.
- **Dividir tareas complejas**: mejora eficiencia y reduce fallos.
- **Monitorear logs y m√©tricas**: usa dashboards como LangSmith o CrewAI UI.
- **Evitar loops innecesarios**: especialmente en AutoGPT o agentes sin control de flujo.

---

## üñºÔ∏è Diapositivas para Canva

### Slide 1: T√≠tulo
**Optimizaci√≥n de Tokens y Evaluaci√≥n de Agentes IA**

### Slide 2: ¬øQu√© es un token?
- Unidad de texto
- Presupuesto por modelo
- Entrada vs salida

### Slide 3: Herramientas de estimaci√≥n
- OpenAI Tokenizer
- tiktoken
- Anthropic Estimator

### Slide 4: Frameworks de agentes
- LangChain: modular
- CrewAI: colaborativo
- AutoGPT: aut√≥nomo

### Slide 5: M√©tricas de eficiencia
- Tokens por tarea
- Latencia
- Tasa de √©xito
- Costo

### Slide 6: Recomendaciones
- Estimar antes de ejecutar
- Dividir tareas
- Monitorear logs
- Evitar loops

---


## üñºÔ∏è Diapositivas para Canva

### Slide 1: T√≠tulo
**Modelos de Razonamiento Avanzado (LRM): Estado del Arte y Benchmarks 2025**

### Slide 2: ¬øQu√© es un LRM?
- Razonamiento l√≥gico
- Uso de herramientas
- Codificaci√≥n y planificaci√≥n

### Slide 3: Benchmarks Clave
- Vending-Bench
- ARC-AGI v2
- LiveCodeBench
- SWE-Bench Verified
- GPQA Diamond

### Slide 4: Modelos L√≠deres
- Grok 4 Heavy
- Claude 4 Sonnet
- GPT-4o
- Gemini 2.5 Pro

### Slide 5: Conclusiones
- Grok 4 supera humanos en m√∫ltiples benchmarks
- Claude 4 lidera en ingenier√≠a de software
- GPQA Diamond redefine el est√°ndar de razonamiento cient√≠fico

### Slide 6: Recomendaciones
- Validar casos de uso antes de adoptar
- No confundir hype con capacidad real
- Supervisi√≥n humana sigue siendo clave

---

## üìå Conclusiones

- **Grok 4 Heavy** establece nuevos est√°ndares en razonamiento, codificaci√≥n y resoluci√≥n de problemas.
- La brecha entre Grok 4 y otros modelos se ampl√≠a en tareas de alta complejidad.
- Aunque Grok 4 lidera en benchmarks, a√∫n enfrenta desaf√≠os en moderaci√≥n, visi√≥n multimodal y confiabilidad √©tica.

---

## üîó Referencias

- [Grok 4 Benchmarks ‚Äì OfficeChai](https://officechai.com/ai/grok4-benchmark-results-how-xais-latest-model-left-openai-google-behind/)
- [Grok 4 Performance Overview ‚Äì LLM Stats](https://llm-stats.com/models/grok-4)
- [Grok 4 Explained ‚Äì Kingy AI](https://kingy.ai/blog/grok-4-benchmarks-explained-why-its-performance-is-a-game-changer/)
- [Grok 4 Heavy Access & Results ‚Äì Mechical](https://www.mechical.com/2025/07/what-is-grok-4-heavy-benchmark-access.html)
- [Grok 4 vs GPT & Gemini ‚Äì Nitro Media Group](https://www.nitromediagroup.com/grok-4-ail-benchmark-leader-2025/)
- [Apple's Study Reveals Insights: Debunking AI Superintelligence Myths](https://www.gravity.global/en/blog/apple-ai-reasoning-myths-debunked)
- [Salesforce and Gartner Cast Doubt on AI Agents](https://www.gravity.global/en/blog/salesforce-and-gartner-cast-doubt-on-ai-agents)
- [Agentic froth not earned, warns Gartner. Studies back that view up.](https://www.thestack.technology/agentic-froth-not-earned-warns-gartner-studies-back-that-view-up/)

---

