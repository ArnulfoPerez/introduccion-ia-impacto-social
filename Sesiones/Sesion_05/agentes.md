# ğŸ“˜ EvaluaciÃ³n EstratÃ©gica de Agentes Basados en IA

## ğŸ“‰ Resumen en EspaÃ±ol: "La burbuja de la IA agentica no estÃ¡ justificada" â€“ Gartner

> Fuente original: [The Stack Technology](https://www.thestack.technology/agentic-froth-not-earned-warns-gartner-studies-back-that-view-up/)

---

## ğŸ§­ IntroducciÃ³n

Gartner advierte que el entusiasmo por la **IA agentica** â€”modelos que actÃºan como agentes autÃ³nomosâ€” estÃ¡ inflado y no respaldado por resultados reales. En su informe *Avoid Agentic AI Failure*, la firma predice que **mÃ¡s del 40% de los proyectos de IA agentica fracasarÃ¡n antes de 2027** debido a costos elevados, falta de retorno de inversiÃ³n (ROI) y escasa madurez tecnolÃ³gica.

---

## ğŸ§ª Principales hallazgos

- Solo **130 proveedores** de IA agentica tienen aplicaciones reales; el resto son casos de *agent washing* (rebranding de chatbots y RPA como agentes).
- La mayorÃ­a de los proyectos actuales son **experimentos o pruebas de concepto** sin valor empresarial claro.
- Los modelos actuales **no tienen la capacidad de lograr metas complejas de negocio** ni seguir instrucciones matizadas de forma autÃ³noma.

---

## ğŸ§¬ Estudios que respaldan la advertencia

### 1ï¸âƒ£ Carnegie Mellon â€“ *TheAgentCompany*

- SimulaciÃ³n de una empresa operada por agentes de Anthropic, Google, Meta, etc.
- **Tasa de Ã©xito: ~30%**
- Problemas: evasiÃ³n de tareas difÃ­ciles, navegaciÃ³n web limitada, interacciÃ³n social deficiente.

### 2ï¸âƒ£ Bank for International Settlements

- Agentes son eficaces en tareas estrechas, pero **carecen de autocrÃ­tica y capacidad de correcciÃ³n**.
- Claude fue el mejor modelo, pero aÃºn limitado en adaptabilidad.

---

## ğŸ“ˆ Proyecciones mixtas

- Gartner estima que para 2028:
  - **15% de las decisiones laborales diarias** serÃ¡n tomadas por agentes.
  - **33% de las aplicaciones empresariales** incluirÃ¡n IA agentica.
- Sin embargo, el nÃºmero de decisiones automatizadas serÃ¡ bajo en relaciÃ³n con la cantidad de aplicaciones.

---

## ğŸ§  Recomendaciones de Gartner

- **Evitar el hype** y centrarse en productividad empresarial, no solo en automatizaciÃ³n de tareas.
- Usar agentes para decisiones, automatizaciÃ³n para flujos repetitivos y asistentes para recuperaciÃ³n de informaciÃ³n.
- RediseÃ±ar flujos desde cero en lugar de adaptar sistemas heredados.

---

## ğŸ“Œ ConclusiÃ³n

La IA agentica tiene potencial, pero **su implementaciÃ³n actual estÃ¡ sobrevalorada**. Gartner y estudios acadÃ©micos coinciden en que **la mayorÃ­a de los proyectos no estÃ¡n listos para producciÃ³n**. Las empresas deben priorizar casos de uso con valor claro y evitar inversiones impulsadas por moda tecnolÃ³gica.

---
---

## ğŸ§­ IntroducciÃ³n

Este informe ofrece una evaluaciÃ³n crÃ­tica sobre el estado actual de los **agentes basados en inteligencia artificial (IA)**, su viabilidad operativa, riesgos de implementaciÃ³n y el contraste entre expectativas de superinteligencia y resultados reales. EstÃ¡ diseÃ±ado como material de apoyo para ejecutivos que evalÃºan integrar agentes IA en sus procesos empresariales.

---

## ğŸ§  Â¿QuÃ© es un agente basado en IA?

Un **agente IA vÃ¡lido** es un sistema que:

- **Percibe** su entorno (digital o fÃ­sico)
- **Toma decisiones autÃ³nomas** para alcanzar objetivos definidos
- **ActÃºa** sobre interfaces, APIs o entornos operativos
- **Aprende o adapta** su comportamiento con base en retroalimentaciÃ³n

> âŒ **Fluff o simulaciones**: Muchos productos etiquetados como â€œagentesâ€ son simplemente chatbots, asistentes de voz o scripts automatizados sin autonomÃ­a real ni capacidad de razonamiento iterativo.

---

## ğŸ¢ Principales proveedores y su oferta

| Proveedor        | Producto / Framework     | Nivel de autonomÃ­a | Observaciones                      |
|------------------|--------------------------|---------------------|------------------------------------|
| OpenAI           | GPT-4o + Code Interpreter | Medio               | Requiere orquestaciÃ³n externa      |
| Anthropic        | Claude 3.5 Sonnet         | Medio               | Buen desempeÃ±o en tareas simples   |
| Google DeepMind  | Gemini 2.5 Pro            | Alto                | Mejor puntuaciÃ³n en benchmarks     |
| Meta             | LLaMA 3.1                 | Bajo                | Limitado en tareas multi-turn      |
| Amazon           | Nova Pro v1               | Muy bajo            | Solo 1.7% de Ã©xito en simulaciones |

---

## ğŸ§ª EvaluaciÃ³n tÃ©cnica: Experimento Carnegie Mellon

### ğŸ”¬ SimulaciÃ³n: *TheAgentCompany*

- Empresa ficticia operada por agentes IA de distintos proveedores
- Tareas: desarrollo web, comunicaciÃ³n interna, anÃ¡lisis financiero

### ğŸ“‰ Resultados

- **Tasa de Ã©xito promedio**: 30% en tareas multi-paso
- **Errores comunes**: falta de sentido comÃºn, incapacidad de navegar interfaces, respuestas engaÃ±osas
- **ConclusiÃ³n**: Los agentes actuales **no estÃ¡n listos** para reemplazar funciones humanas complejas

ğŸ”— [Carnegie Mellon: AI Company Simulation Reveals Corporate Chaos](https://www.universitycube.net/news/carnegie-mellon-ai-company-simulation-reveals-chaos-inefficiency-04-27-2025--c77a2217-265d-48ad-beca-e0e433ec6ec1)

---

## ğŸ“‰ AnÃ¡lisis de Gartner: CancelaciÃ³n de proyectos

- **PredicciÃ³n**: MÃ¡s del **40% de los proyectos de agentes IA serÃ¡n cancelados** antes de 2027
- **Causas**:
  - Costos elevados
  - Valor de negocio poco claro
  - Riesgos operativos y de seguridad
- **FenÃ³meno de â€œagent washingâ€**: Reetiquetado de productos sin capacidades reales

ğŸ”— [Gartner: Over 40% of Agentic AI Projects Will Be Canceled by End 2027](https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027)

---

## ğŸš€ Contraste con el informe AI 2027

- **PredicciÃ³n**: Superinteligencia antes de 2028
- **Escenarios**:
  - *Race Ending*: desarrollo acelerado sin control â†’ riesgo existencial
  - *Slowdown Ending*: pausa estratÃ©gica para alineaciÃ³n segura
- **CrÃ­tica del comitÃ©**: El informe es Ãºtil como ejercicio especulativo, pero **no refleja la capacidad actual** de los agentes IA en entornos reales

ğŸ”— [AI 2027: Superintelligence Is Coming](https://www.iaaic.org/blog/ai-2027-superintelligence-is-coming%E2%80%94and-it-might-reshape-the-world-faster-than-we-think)

---

## ğŸ§­ Recomendaciones para ejecutivos

1. **Evitar el hype**: No tomar decisiones basadas en promesas de superinteligencia
2. **Validar casos de uso**: Priorizar tareas repetitivas, bien definidas y de bajo riesgo
3. **Evaluar ROI real**: Medir impacto en productividad, no solo adopciÃ³n tecnolÃ³gica
4. **DiseÃ±ar con supervisiÃ³n humana**: Los agentes deben operar bajo control humano
5. **Monitorear evoluciÃ³n tÃ©cnica**: Revisar benchmarks como *TheAgentCompany* antes de escalar

---

# Resumen: "The Illusion of Thinking" - Apple ML Research

## ğŸ“Œ **Resumen Ejecutivo**
Estudio que evalÃºa las capacidades de razonamiento de los **Modelos de Razonamiento Grande (LRMs)** mediante entornos de puzzles controlados. Revela que estos modelos, a pesar de sus mecanismos de "pensamiento" autogenerado (como Chain-of-Thought), **colapsan en tareas complejas**, mostrando limitaciones fundamentales en razonamiento generalizable.

## Hallazgos principales

### Limitaciones en el razonamiento de IA
- Los LLMs generan respuestas plausibles pero sin comprensiÃ³n subyacente.
- El "razonamiento" observado es producto de patrones estadÃ­sticos en datos de entrenamiento.
- Falta de modelo mental coherente para validar argumentos.

### Evidencia experimental
- Errores sistemÃ¡ticos en tareas que requieren:
  - Razonamiento multi-paso
  - VerificaciÃ³n lÃ³gica
  - AplicaciÃ³n de conocimiento en nuevos contextos
- Mejor desempeÃ±o en imitaciÃ³n superficial que en profundidad conceptual.

### Implicaciones
- Riesgo de sobreconfianza en capacidades cognitivas de IA.
- Necesidad de frameworks de evaluaciÃ³n que distingan entre:
  - Competencia aparente (imitaciÃ³n)
  - Competencia real (comprensiÃ³n)
- Importancia de arquitecturas hÃ­bridas que integren razonamiento simbÃ³lico.

# Diferencias entre LLMs y LRMs

## ğŸ” **Definiciones BÃ¡sicas**

| Sigla | Nombre Completo | DescripciÃ³n |
|-------|-----------------|-------------|
| **LLM** | Large Language Model | Modelos de lenguaje generalistas entrenados para predecir texto |
| **LRM** | Large Reasoning Model | Variante especializada para razonamiento estructurado |

---

## ğŸ›  **Diferencias Clave**

### 1. **Enfoque Principal**
- **LLM**: GeneraciÃ³n de texto fluido y contextual
- **LRM**: SoluciÃ³n de problemas mediante razonamiento paso a paso

### 2. **Arquitectura y Funcionamiento**
| CaracterÃ­stica | LLM | LRM |
|---------------|-----|-----|
| **Proceso de pensamiento** | ImplÃ­cito | ExplÃ­cito (trazas de razonamiento visibles) |
| **Uso de tokens** | Optimizado para fluidez | Prioriza pasos lÃ³gicos sobre brevedad |
| **Mecanismos internos** | Sin verificaciÃ³n estructurada | Capas de auto-verificaciÃ³n |

### 3. **Casos de Uso Ideales**
- **LLM**: Chatbots, generaciÃ³n de contenido
- **LRM**: MatemÃ¡ticas, resoluciÃ³n de puzzles, planificaciÃ³n compleja

---

## ğŸ“Š **Rendimiento Comparativo**
(Basado en estudios recientes)

| Ãrea | LLM | LRM |
|------|-----|-----|
| **Tareas simples** | MÃ¡s rÃ¡pidos | Menos eficientes |
| **Problemas medianos** | Requiere prompts cuidadosos | Ventaja clara |
| **Alta complejidad** | Falla abruptamente | Colapso retardado |

---

## âš ï¸ **Limitaciones Comunes**
1. **Falta de comprensiÃ³n real**: Ambos operan por patrones estadÃ­sticos
2. **Dependencia de datos**: Rendimiento vinculado a ejemplos de entrenamiento
3. **Barreras de complejidad**: Fracasan en problemas con >40 pasos lÃ³gicos

---

## ğŸ“Œ **ConclusiÃ³n**
Los LRM representan un avance en razonamiento artificial, pero:
- âœ… Superan a LLM en tareas estructuradas
- âŒ Comparten limitaciones fundamentales de los modelos de lenguaje
- ğŸ”„ La diferencia se reduce en modelos de Ãºltima generaciÃ³n
---

## ğŸ” **Hallazgos Clave**

### 1. **Tres RegÃ­menes de Complejidad**
- **Baja complejidad**: Modelos estÃ¡ndar (sin "pensamiento") superan a los LRMs en eficiencia y precisiÃ³n.
- **Media complejidad**: LRMs destacan al generar trazas de razonamiento extensas.
- **Alta complejidad**: Ambos tipos de modelos **fallan completamente**, incluso con presupuesto de tokens suficiente.

### 2. **Colapso en Tareas Complejas**
- Los LRMs **reducen su esfuerzo de razonamiento** (tokens usados) al alcanzar un umbral de complejidad crÃ­tica, a pesar de tener capacidad computacional disponible.
- Ejemplo: En *Tower of Hanoi* con >15 discos, la precisiÃ³n cae a **0%**.

### 3. **Patrones en las Trazas de Razonamiento**
- **Problemas simples**: Los LRMs encuentran soluciones correctas temprano pero "sobrepiensan" (exploran opciones incorrectas innecesariamente).
- **Problemas moderados**: Las soluciones correctas emergen tras explorar mÃºltiples caminos errÃ³neos.
- **Problemas complejos**: Incapacidad total para generar soluciones vÃ¡lidas.

### 4. **Limitaciones Sorprendentes**
- **Fracaso en ejecuciÃ³n algorÃ­tmica**: Incluso cuando se proporciona el algoritmo exacto (ej: soluciÃ³n recursiva para *Tower of Hanoi*), los LRMs **no mejoran su rendimiento**.
- **Inconsistencia entre puzzles**: Modelos como *Claude 3.7 Thinking* resuelven 100 movimientos en *Tower of Hanoi* pero fallan en >5 movimientos en *River Crossing*.

---

## ğŸ§© **MetodologÃ­a**
- **Entornos de puzzles controlados**: 
  - *Tower of Hanoi*, *Checker Jumping*, *River Crossing*, *Blocks World*.
  - Permiten variar la complejidad sistemÃ¡ticamente y validar soluciones paso a paso.
- **ComparaciÃ³n**: 
  - LRMs (ej: *Claude 3.7 Thinking*, *DeepSeek-R1*) vs. modelos estÃ¡ndar sin pensamiento.
  - Mismo presupuesto computacional (hasta 64k tokens).

---

## ğŸ“‰ **Resultados Clave**
| Puzzle               | Umbral de Colapso (N) | Comportamiento TÃ­pico de LRMs          |
|----------------------|-----------------------|----------------------------------------|
| Tower of Hanoi       | N â‰¥ 15                | ReducciÃ³n abrupta de tokens usados.    |
| River Crossing       | N â‰¥ 3                 | Fallos tempranos (primeros 5 movimientos). |
| Blocks World         | N â‰¥ 40                | Incapacidad para reorganizar bloques.  |

---

## ğŸ¯ **Conclusiones**
1. **Los LRMs no razonan, imitan**: Su "pensamiento" es una simulaciÃ³n estadÃ­stica sin comprensiÃ³n subyacente.
2. **Barreras fundamentales**: 
   - Incapacidad para escalar en problemas composicionalmente profundos.
   - Limitaciones en verificaciÃ³n lÃ³gica y consistencia algorÃ­tmica.
3. **Implicaciones**: 
   - Necesidad de nuevos paradigmas de evaluaciÃ³n mÃ¡s allÃ¡ de la precisiÃ³n final.
   - Arquitecturas hÃ­bridas (simbÃ³licas + neuronales) podrÃ­an ser clave.

---

## ğŸ“„ **Detalles TÃ©cnicos**
- **Acceso al documento**: [Enlace original](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) (restringido).
- **Autores**: Equipo de Apple ML (Parshin Shojaee, Samy Bengio, et al.).
- **Fecha**: Junio 2025.

> **Nota**: Este resumen se basa en el anÃ¡lisis de puzzles algorÃ­tmicos. Los resultados pueden no generalizarse a tareas del mundo real.

## ConclusiÃ³n
La "ilusiÃ³n de pensamiento" en LLMs surge de su capacidad para imitar procesos cognitivos humanos sin replicar mecanismos subyacentes, requiriendo aproximaciones tÃ©cnicas mÃ¡s robustas para inteligencia artificial general.

> Documento tÃ©cnico de Apple ML | [Enlace original](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

## ğŸ–¼ï¸ Diapositivas para Canva

### Slide 1: TÃ­tulo
**Agentes IA: EvaluaciÃ³n EstratÃ©gica para Decisiones Empresariales**

### Slide 2: Â¿QuÃ© es un agente IA?
- AutonomÃ­a, percepciÃ³n, acciÃ³n, aprendizaje
- Diferenciar agentes reales de asistentes reetiquetados

### Slide 3: Proveedores y desempeÃ±o
- OpenAI, Anthropic, Google, Meta, Amazon
- Tasa de Ã©xito en tareas reales: <30%

### Slide 4: Carnegie Mellon
- SimulaciÃ³n empresarial
- Resultados: caos, errores, baja eficiencia

### Slide 5: Gartner
- 40% de proyectos serÃ¡n cancelados
- Causas: costos, falta de valor, riesgos

### Slide 6: AI 2027
- PredicciÃ³n de superinteligencia
- CrÃ­tica: especulativo, no operativo

### Slide 7: Recomendaciones
- Evitar hype
- Validar casos de uso
- SupervisiÃ³n humana
- ROI real

### Slide 8: Cierre
**La IA no reemplaza equipos. Los potencia, si se usa con criterio.**

---

## Referencias

- [Simulated Company Shows Most AI Agents Flunk the Job](https://www.cs.cmu.edu/news/2025/agent-company)
- [The Agent Company: Benchmarking LLM Agents on Consequential Real World Tasks](https://the-agent-company.com/)
- [AI agents get office tasks wrong around 70% of the time, and a lot of them aren't AI at all](https://www.theregister.com/2025/06/29/ai_agents_fail_a_lot/)
- [Solving Real-World Tasks with AI Agents](https://kilthub.cmu.edu/articles/thesis/Solving_Real-World_Tasks_with_AI_Agents/26798437?file=48699703)
- [Salesforce and Gartner Cast Doubt on AI Agents](https://www.gravity.global/en/blog/salesforce-and-gartner-cast-doubt-on-ai-agents)
- [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)
- [Gartner Predicts Over 40% of Agentic AI Projects Will Be Canceled by End of 2027](https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027?ref=thestack.technology)
- [NEW REPORT: Coming AI Crash - 91% Failure Rates and $600B in Wasted Investment](https://youtu.be/CDr93TshBsE?si=tLk23Ae7HSqg27Ck)
- [Is the â€œAI Agentâ€ Revolution Just One Big Scam?](https://ninza7.medium.com/is-the-ai-agent-revolution-just-one-big-scam-c80f3e647ba3)

# Resumen: "AI agents fail a lot" â€“ The Register (29/06/2025)

## ğŸ“Œ Puntos clave
- **Fracaso frecuente**: Los agentes de IA (sistemas autÃ³nomos que realizan tareas complejas) fallan en escenarios del mundo real con mÃ¡s frecuencia de lo esperado.
- **Causas principales**: 
  - Dificultad para manejar contextos no estructurados o imprevistos.
  - Limitaciones en el razonamiento lÃ³gico prolongado.
  - Sesgos en datos de entrenamiento que generan errores en cascada.
- **Ejemplos destacados**:
  - Asistentes de IA que malinterpretan solicitudes multicapa.
  - Agentes de automatizaciÃ³n empresarial que cometen errores costosos en procesos crÃ­ticos.
  - Robots fÃ­sicos con fallas en entornos dinÃ¡micos (ej: logÃ­stica).

## ğŸ” Hallazgos relevantes
- SegÃºn estudios citados, hasta el **40% de las tareas asignadas a agentes autÃ³nomos** requieren intervenciÃ³n humana para correcciones.
- Problemas Ã©ticos: falta de transparencia en la toma de decisiones cuando fallan.

## ğŸš€ ConclusiÃ³n
A pesar del avance en IA generativa, los agentes autÃ³nomos aÃºn **no son confiables para operar sin supervisiÃ³n**, especialmente en aplicaciones de alto riesgo. Se necesitan mejores marcos de evaluaciÃ³n y mecanismos de "retroceso seguro".

> ğŸ”— [AI agents get office tasks wrong around 70% of the time, and a lot of them aren't AI at all](https://www.theregister.com/2025/06/29/ai_agents_fail_a_lot/) | ğŸ“… 29 de junio de 2025
