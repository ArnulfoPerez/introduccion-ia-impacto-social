# 📘 Evaluación Estratégica de Agentes Basados en IA

## 📉 Resumen en Español: "La burbuja de la IA agentica no está justificada" – Gartner

> Fuente original: [The Stack Technology](https://www.thestack.technology/agentic-froth-not-earned-warns-gartner-studies-back-that-view-up/)

---

## 🧭 Introducción

Gartner advierte que el entusiasmo por la **IA agentica** —modelos que actúan como agentes autónomos— está inflado y no respaldado por resultados reales. En su informe *Avoid Agentic AI Failure*, la firma predice que **más del 40% de los proyectos de IA agentica fracasarán antes de 2027** debido a costos elevados, falta de retorno de inversión (ROI) y escasa madurez tecnológica.

---

## 🧪 Principales hallazgos

- Solo **130 proveedores** de IA agentica tienen aplicaciones reales; el resto son casos de *agent washing* (rebranding de chatbots y RPA como agentes).
- La mayoría de los proyectos actuales son **experimentos o pruebas de concepto** sin valor empresarial claro.
- Los modelos actuales **no tienen la capacidad de lograr metas complejas de negocio** ni seguir instrucciones matizadas de forma autónoma.

---

## 🧬 Estudios que respaldan la advertencia

### 1️⃣ Carnegie Mellon – *TheAgentCompany*

- Simulación de una empresa operada por agentes de Anthropic, Google, Meta, etc.
- **Tasa de éxito: ~30%**
- Problemas: evasión de tareas difíciles, navegación web limitada, interacción social deficiente.

### 2️⃣ Bank for International Settlements

- Agentes son eficaces en tareas estrechas, pero **carecen de autocrítica y capacidad de corrección**.
- Claude fue el mejor modelo, pero aún limitado en adaptabilidad.

---

## 📈 Proyecciones mixtas

- Gartner estima que para 2028:
  - **15% de las decisiones laborales diarias** serán tomadas por agentes.
  - **33% de las aplicaciones empresariales** incluirán IA agentica.
- Sin embargo, el número de decisiones automatizadas será bajo en relación con la cantidad de aplicaciones.

---

## 🧠 Recomendaciones de Gartner

- **Evitar el hype** y centrarse en productividad empresarial, no solo en automatización de tareas.
- Usar agentes para decisiones, automatización para flujos repetitivos y asistentes para recuperación de información.
- Rediseñar flujos desde cero en lugar de adaptar sistemas heredados.

---

## 📌 Conclusión

La IA agentica tiene potencial, pero **su implementación actual está sobrevalorada**. Gartner y estudios académicos coinciden en que **la mayoría de los proyectos no están listos para producción**. Las empresas deben priorizar casos de uso con valor claro y evitar inversiones impulsadas por moda tecnológica.

---
---

## 🧭 Introducción

Este informe ofrece una evaluación crítica sobre el estado actual de los **agentes basados en inteligencia artificial (IA)**, su viabilidad operativa, riesgos de implementación y el contraste entre expectativas de superinteligencia y resultados reales. Está diseñado como material de apoyo para ejecutivos que evalúan integrar agentes IA en sus procesos empresariales.

---

## 🧠 ¿Qué es un agente basado en IA?

Un **agente IA válido** es un sistema que:

- **Percibe** su entorno (digital o físico)
- **Toma decisiones autónomas** para alcanzar objetivos definidos
- **Actúa** sobre interfaces, APIs o entornos operativos
- **Aprende o adapta** su comportamiento con base en retroalimentación

> ❌ **Fluff o simulaciones**: Muchos productos etiquetados como “agentes” son simplemente chatbots, asistentes de voz o scripts automatizados sin autonomía real ni capacidad de razonamiento iterativo.

---

## 🏢 Principales proveedores y su oferta

| Proveedor        | Producto / Framework     | Nivel de autonomía | Observaciones                      |
|------------------|--------------------------|---------------------|------------------------------------|
| OpenAI           | GPT-4o + Code Interpreter | Medio               | Requiere orquestación externa      |
| Anthropic        | Claude 3.5 Sonnet         | Medio               | Buen desempeño en tareas simples   |
| Google DeepMind  | Gemini 2.5 Pro            | Alto                | Mejor puntuación en benchmarks     |
| Meta             | LLaMA 3.1                 | Bajo                | Limitado en tareas multi-turn      |
| Amazon           | Nova Pro v1               | Muy bajo            | Solo 1.7% de éxito en simulaciones |

---

## 🧪 Evaluación técnica: Experimento Carnegie Mellon

### 🔬 Simulación: *TheAgentCompany*

- Empresa ficticia operada por agentes IA de distintos proveedores
- Tareas: desarrollo web, comunicación interna, análisis financiero

### 📉 Resultados

- **Tasa de éxito promedio**: 30% en tareas multi-paso
- **Errores comunes**: falta de sentido común, incapacidad de navegar interfaces, respuestas engañosas
- **Conclusión**: Los agentes actuales **no están listos** para reemplazar funciones humanas complejas

🔗 [Carnegie Mellon: AI Company Simulation Reveals Corporate Chaos](https://www.universitycube.net/news/carnegie-mellon-ai-company-simulation-reveals-chaos-inefficiency-04-27-2025--c77a2217-265d-48ad-beca-e0e433ec6ec1)

---

## 📉 Análisis de Gartner: Cancelación de proyectos

- **Predicción**: Más del **40% de los proyectos de agentes IA serán cancelados** antes de 2027
- **Causas**:
  - Costos elevados
  - Valor de negocio poco claro
  - Riesgos operativos y de seguridad
- **Fenómeno de “agent washing”**: Reetiquetado de productos sin capacidades reales

🔗 [Gartner: Over 40% of Agentic AI Projects Will Be Canceled by End 2027](https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027)

---

## 🚀 Contraste con el informe AI 2027

- **Predicción**: Superinteligencia antes de 2028
- **Escenarios**:
  - *Race Ending*: desarrollo acelerado sin control → riesgo existencial
  - *Slowdown Ending*: pausa estratégica para alineación segura
- **Crítica del comité**: El informe es útil como ejercicio especulativo, pero **no refleja la capacidad actual** de los agentes IA en entornos reales

🔗 [AI 2027: Superintelligence Is Coming](https://www.iaaic.org/blog/ai-2027-superintelligence-is-coming%E2%80%94and-it-might-reshape-the-world-faster-than-we-think)

---

## 🧭 Recomendaciones para ejecutivos

1. **Evitar el hype**: No tomar decisiones basadas en promesas de superinteligencia
2. **Validar casos de uso**: Priorizar tareas repetitivas, bien definidas y de bajo riesgo
3. **Evaluar ROI real**: Medir impacto en productividad, no solo adopción tecnológica
4. **Diseñar con supervisión humana**: Los agentes deben operar bajo control humano
5. **Monitorear evolución técnica**: Revisar benchmarks como *TheAgentCompany* antes de escalar

---

# Resumen: "The Illusion of Thinking" - Apple ML Research

## 📌 **Resumen Ejecutivo**
Estudio que evalúa las capacidades de razonamiento de los **Modelos de Razonamiento Grande (LRMs)** mediante entornos de puzzles controlados. Revela que estos modelos, a pesar de sus mecanismos de "pensamiento" autogenerado (como Chain-of-Thought), **colapsan en tareas complejas**, mostrando limitaciones fundamentales en razonamiento generalizable.

## Hallazgos principales

### Limitaciones en el razonamiento de IA
- Los LLMs generan respuestas plausibles pero sin comprensión subyacente.
- El "razonamiento" observado es producto de patrones estadísticos en datos de entrenamiento.
- Falta de modelo mental coherente para validar argumentos.

### Evidencia experimental
- Errores sistemáticos en tareas que requieren:
  - Razonamiento multi-paso
  - Verificación lógica
  - Aplicación de conocimiento en nuevos contextos
- Mejor desempeño en imitación superficial que en profundidad conceptual.

### Implicaciones
- Riesgo de sobreconfianza en capacidades cognitivas de IA.
- Necesidad de frameworks de evaluación que distingan entre:
  - Competencia aparente (imitación)
  - Competencia real (comprensión)
- Importancia de arquitecturas híbridas que integren razonamiento simbólico.

# Diferencias entre LLMs y LRMs

## 🔍 **Definiciones Básicas**

| Sigla | Nombre Completo | Descripción |
|-------|-----------------|-------------|
| **LLM** | Large Language Model | Modelos de lenguaje generalistas entrenados para predecir texto |
| **LRM** | Large Reasoning Model | Variante especializada para razonamiento estructurado |

---

## 🛠 **Diferencias Clave**

### 1. **Enfoque Principal**
- **LLM**: Generación de texto fluido y contextual
- **LRM**: Solución de problemas mediante razonamiento paso a paso

### 2. **Arquitectura y Funcionamiento**
| Característica | LLM | LRM |
|---------------|-----|-----|
| **Proceso de pensamiento** | Implícito | Explícito (trazas de razonamiento visibles) |
| **Uso de tokens** | Optimizado para fluidez | Prioriza pasos lógicos sobre brevedad |
| **Mecanismos internos** | Sin verificación estructurada | Capas de auto-verificación |

### 3. **Casos de Uso Ideales**
- **LLM**: Chatbots, generación de contenido
- **LRM**: Matemáticas, resolución de puzzles, planificación compleja

---

## 📊 **Rendimiento Comparativo**
(Basado en estudios recientes)

| Área | LLM | LRM |
|------|-----|-----|
| **Tareas simples** | Más rápidos | Menos eficientes |
| **Problemas medianos** | Requiere prompts cuidadosos | Ventaja clara |
| **Alta complejidad** | Falla abruptamente | Colapso retardado |

---

## ⚠️ **Limitaciones Comunes**
1. **Falta de comprensión real**: Ambos operan por patrones estadísticos
2. **Dependencia de datos**: Rendimiento vinculado a ejemplos de entrenamiento
3. **Barreras de complejidad**: Fracasan en problemas con >40 pasos lógicos

---

## 📌 **Conclusión**
Los LRM representan un avance en razonamiento artificial, pero:
- ✅ Superan a LLM en tareas estructuradas
- ❌ Comparten limitaciones fundamentales de los modelos de lenguaje
- 🔄 La diferencia se reduce en modelos de última generación
---

## 🔍 **Hallazgos Clave**

### 1. **Tres Regímenes de Complejidad**
- **Baja complejidad**: Modelos estándar (sin "pensamiento") superan a los LRMs en eficiencia y precisión.
- **Media complejidad**: LRMs destacan al generar trazas de razonamiento extensas.
- **Alta complejidad**: Ambos tipos de modelos **fallan completamente**, incluso con presupuesto de tokens suficiente.

### 2. **Colapso en Tareas Complejas**
- Los LRMs **reducen su esfuerzo de razonamiento** (tokens usados) al alcanzar un umbral de complejidad crítica, a pesar de tener capacidad computacional disponible.
- Ejemplo: En *Tower of Hanoi* con >15 discos, la precisión cae a **0%**.

### 3. **Patrones en las Trazas de Razonamiento**
- **Problemas simples**: Los LRMs encuentran soluciones correctas temprano pero "sobrepiensan" (exploran opciones incorrectas innecesariamente).
- **Problemas moderados**: Las soluciones correctas emergen tras explorar múltiples caminos erróneos.
- **Problemas complejos**: Incapacidad total para generar soluciones válidas.

### 4. **Limitaciones Sorprendentes**
- **Fracaso en ejecución algorítmica**: Incluso cuando se proporciona el algoritmo exacto (ej: solución recursiva para *Tower of Hanoi*), los LRMs **no mejoran su rendimiento**.
- **Inconsistencia entre puzzles**: Modelos como *Claude 3.7 Thinking* resuelven 100 movimientos en *Tower of Hanoi* pero fallan en >5 movimientos en *River Crossing*.

---

## 🧩 **Metodología**
- **Entornos de puzzles controlados**: 
  - *Tower of Hanoi*, *Checker Jumping*, *River Crossing*, *Blocks World*.
  - Permiten variar la complejidad sistemáticamente y validar soluciones paso a paso.
- **Comparación**: 
  - LRMs (ej: *Claude 3.7 Thinking*, *DeepSeek-R1*) vs. modelos estándar sin pensamiento.
  - Mismo presupuesto computacional (hasta 64k tokens).

---

## 📉 **Resultados Clave**
| Puzzle               | Umbral de Colapso (N) | Comportamiento Típico de LRMs          |
|----------------------|-----------------------|----------------------------------------|
| Tower of Hanoi       | N ≥ 15                | Reducción abrupta de tokens usados.    |
| River Crossing       | N ≥ 3                 | Fallos tempranos (primeros 5 movimientos). |
| Blocks World         | N ≥ 40                | Incapacidad para reorganizar bloques.  |

---

## 🎯 **Conclusiones**
1. **Los LRMs no razonan, imitan**: Su "pensamiento" es una simulación estadística sin comprensión subyacente.
2. **Barreras fundamentales**: 
   - Incapacidad para escalar en problemas composicionalmente profundos.
   - Limitaciones en verificación lógica y consistencia algorítmica.
3. **Implicaciones**: 
   - Necesidad de nuevos paradigmas de evaluación más allá de la precisión final.
   - Arquitecturas híbridas (simbólicas + neuronales) podrían ser clave.

---

## 📄 **Detalles Técnicos**
- **Acceso al documento**: [Enlace original](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) (restringido).
- **Autores**: Equipo de Apple ML (Parshin Shojaee, Samy Bengio, et al.).
- **Fecha**: Junio 2025.

> **Nota**: Este resumen se basa en el análisis de puzzles algorítmicos. Los resultados pueden no generalizarse a tareas del mundo real.

## Conclusión
La "ilusión de pensamiento" en LLMs surge de su capacidad para imitar procesos cognitivos humanos sin replicar mecanismos subyacentes, requiriendo aproximaciones técnicas más robustas para inteligencia artificial general.

> Documento técnico de Apple ML | [Enlace original](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

## 🖼️ Diapositivas para Canva

### Slide 1: Título
**Agentes IA: Evaluación Estratégica para Decisiones Empresariales**

### Slide 2: ¿Qué es un agente IA?
- Autonomía, percepción, acción, aprendizaje
- Diferenciar agentes reales de asistentes reetiquetados

### Slide 3: Proveedores y desempeño
- OpenAI, Anthropic, Google, Meta, Amazon
- Tasa de éxito en tareas reales: <30%

### Slide 4: Carnegie Mellon
- Simulación empresarial
- Resultados: caos, errores, baja eficiencia

### Slide 5: Gartner
- 40% de proyectos serán cancelados
- Causas: costos, falta de valor, riesgos

### Slide 6: AI 2027
- Predicción de superinteligencia
- Crítica: especulativo, no operativo

### Slide 7: Recomendaciones
- Evitar hype
- Validar casos de uso
- Supervisión humana
- ROI real

### Slide 8: Cierre
**La IA no reemplaza equipos. Los potencia, si se usa con criterio.**

---

## Referencias

- [Simulated Company Shows Most AI Agents Flunk the Job](https://www.cs.cmu.edu/news/2025/agent-company)
- [The Agent Company: Benchmarking LLM Agents on Consequential Real World Tasks](https://the-agent-company.com/)
- [AI agents get office tasks wrong around 70% of the time, and a lot of them aren't AI at all](https://www.theregister.com/2025/06/29/ai_agents_fail_a_lot/)
- [Solving Real-World Tasks with AI Agents](https://kilthub.cmu.edu/articles/thesis/Solving_Real-World_Tasks_with_AI_Agents/26798437?file=48699703)
- [Salesforce and Gartner Cast Doubt on AI Agents](https://www.gravity.global/en/blog/salesforce-and-gartner-cast-doubt-on-ai-agents)
- [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)
- [Gartner Predicts Over 40% of Agentic AI Projects Will Be Canceled by End of 2027](https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027?ref=thestack.technology)
- [NEW REPORT: Coming AI Crash - 91% Failure Rates and $600B in Wasted Investment](https://youtu.be/CDr93TshBsE?si=tLk23Ae7HSqg27Ck)
- [Is the “AI Agent” Revolution Just One Big Scam?](https://ninza7.medium.com/is-the-ai-agent-revolution-just-one-big-scam-c80f3e647ba3)

# Resumen: "AI agents fail a lot" – The Register (29/06/2025)

## 📌 Puntos clave
- **Fracaso frecuente**: Los agentes de IA (sistemas autónomos que realizan tareas complejas) fallan en escenarios del mundo real con más frecuencia de lo esperado.
- **Causas principales**: 
  - Dificultad para manejar contextos no estructurados o imprevistos.
  - Limitaciones en el razonamiento lógico prolongado.
  - Sesgos en datos de entrenamiento que generan errores en cascada.
- **Ejemplos destacados**:
  - Asistentes de IA que malinterpretan solicitudes multicapa.
  - Agentes de automatización empresarial que cometen errores costosos en procesos críticos.
  - Robots físicos con fallas en entornos dinámicos (ej: logística).

## 🔍 Hallazgos relevantes
- Según estudios citados, hasta el **40% de las tareas asignadas a agentes autónomos** requieren intervención humana para correcciones.
- Problemas éticos: falta de transparencia en la toma de decisiones cuando fallan.

## 🚀 Conclusión
A pesar del avance en IA generativa, los agentes autónomos aún **no son confiables para operar sin supervisión**, especialmente en aplicaciones de alto riesgo. Se necesitan mejores marcos de evaluación y mecanismos de "retroceso seguro".

> 🔗 [AI agents get office tasks wrong around 70% of the time, and a lot of them aren't AI at all](https://www.theregister.com/2025/06/29/ai_agents_fail_a_lot/) | 📅 29 de junio de 2025
