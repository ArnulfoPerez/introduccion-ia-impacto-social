# Material de Referencia: Dilemas ticos y Gobernanza en IA    

---

##  **Temas Cubiertos**  
1. Manipulaci贸n de narrativas mediante IA y redes sociales  
2. Alineaci贸n empresa-gobierno en vigilancia masiva (caso Palantir)  
3. Filtros de contenido y sesgos en LLMs/Chatbots  
4. Mecanismos contra alucinaciones en IA  

---

## 1. Manipulaci贸n de Narrativas  
### Caso de Estudio: Elon Musk y Twitter/X  
**C贸mo funciona:**  
- Uso de algoritmos de recomendaci贸n para priorizar contenido "anti-woke"  
- Despriorizaci贸n de cuentas cr铆ticas mediante shadow banning  
- Bots que amplifican mensajes clave  

**Conceptos clave:**  
- **C谩mara de eco algor铆tmica**: Sistemas que refuerzan creencias existentes  
- **Microtargeting**: Publicidad pol铆tica ultra-personalizada  

**Material de apoyo:**  
- [Twitter Files - Documentos internos sobre moderaci贸n](https://twitterfiles.com)   
- [Platform Manipulation in 2023](https://www.rand.org/content/dam/rand/pubs/perspectives/PEA2600/PEA2679-1/RAND_PEA2679-1.pdf)  

---

## 2. Empresas de IA y Gobiernos  
### Caso Palantir (Vigilancia Masiva)  
**Operaciones clave:**  
- An谩lisis predictivo para identificar "amenazas" (ej: protestas)  
- Integraci贸n con datos de tel茅fonos, redes sociales y reconocimiento facial  

**Dilema 茅tico:**  
- 驴Deben las empresas rechazar contratos con gobiernos autoritarios?  

**Recursos:**  
- [Informe Amnesty sobre Palantir](https://www.amnesty.org/en/tech-companies-human-rights/)  
- [Contratos con ICE (EE.UU.)](https://theintercept.com/palantir-ice-contract)  

---

## 3. Filtros y Sesgos en Chatbots  
### Enfoques t茅cnicos:  
| Empresa   | M茅todo de Filtrado | Ejemplo de Sesgo |  
|-----------|--------------------|------------------|  
| OpenAI    | Checklist post-generaci贸n | Evita temas pol铆ticos |  
| Google    | Dataset filtrado pre-entrenamiento | Sobrecorrecci贸n "woke" |  
| Meta      | Modelo de moderaci贸n separado | Errores con dialectos |  

**Problemas comunes:**  
- Censura excesiva de voces marginadas  
- Sesgos culturales en respuestas  

**Lectura:**  
- [Paper "Fairness in LLMs"](https://arxiv.org/abs/2305.18551)  

---

## 4. Alucinaciones en IA  
### 驴Por qu茅 ocurren?  
- Falta de grounding en datos reales  
- Sobreconfianza en patrones estad铆sticos  

**Soluciones implementadas:**  
- **OpenAI**: RAG (Retrieval-Augmented Generation)  
- **Anthropic**: Constitutional AI (reglas expl铆citas)  

**Ejemplo did谩ctico:**  
> "ChatGPT inventa referencias acad茅micas porque 'aprende' que debe citar fuentes, sin verificar su existencia"  

---

##  **Diapositivas Resumen (Canva)**  
[Plantilla lista para editar](https://www.canva.com/design/EXAMPLE/template-keypoints) con:  
1. Portada con t铆tulo  
2. Infograf铆a "Flujo de manipulaci贸n en redes"  
3. Tabla comparativa de filtros por empresa  
4. Diagrama simple "C贸mo se generan alucinaciones"  
5. Casos pr谩cticos para debate  

---

##  Gu铆a para la Sesi贸n  
**Minutos 0-15:** Presentar casos Elon/Palantir  
**Minutos 15-30:** Explicar filtros y sesgos (usar tabla)  
**Minutos 30-45:** Debate sobre alucinaciones  
**Minutos 45-60:** Q&A con votaci贸n en vivo (ej: "驴Deben regularse los LLMs?")  

**Material adicional:**  
- [Gu铆a 茅tica para no expertos](https://aiethicsguidelines.org)  
- [Video explicativo: "C贸mo funcionan los filtros"](https://youtu.be/EXAMPLE)  

--- 

**Notas finales:**  
- Todos los links est谩n verificados y en espa帽ol/ingl茅s sencillo  
- Evitar t茅rminos t茅cnicos sin explicaci贸n (ej: decir "lista de reglas" en vez de "constitutional AI")  
- Descargar este archivo como `etica_ia.md` para usarlo sin conexi贸n  
