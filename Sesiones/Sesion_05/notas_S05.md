# Material de Referencia: Dilemas Éticos y Gobernanza en IA    

---

## 🌐 **Temas Cubiertos**  
1. Manipulación de narrativas mediante IA y redes sociales  
2. Alineación empresa-gobierno en vigilancia masiva (caso Palantir)  
3. Filtros de contenido y sesgos en LLMs/Chatbots  
4. Mecanismos contra alucinaciones en IA  

---

## 1. Manipulación de Narrativas  
### Caso de Estudio: Elon Musk y Twitter/X  
**Cómo funciona:**  
- Uso de algoritmos de recomendación para priorizar contenido "anti-woke"  
- Despriorización de cuentas críticas mediante shadow banning  
- Bots que amplifican mensajes clave  

**Conceptos clave:**  
- **Cámara de eco algorítmica**: Sistemas que refuerzan creencias existentes  
- **Microtargeting**: Publicidad política ultra-personalizada  

**Material de apoyo:**  
- [Twitter Files - Documentos internos sobre moderación](https://twitterfiles.com)   
- [Platform Manipulation in 2023](https://www.rand.org/content/dam/rand/pubs/perspectives/PEA2600/PEA2679-1/RAND_PEA2679-1.pdf)  

---

## 2. Empresas de IA y Gobiernos  
### Caso Palantir (Vigilancia Masiva)  
**Operaciones clave:**  
- Análisis predictivo para identificar "amenazas" (ej: protestas)  
- Integración con datos de teléfonos, redes sociales y reconocimiento facial  

**Dilema ético:**  
- ¿Deben las empresas rechazar contratos con gobiernos autoritarios?  

**Recursos:**  
- [Informe Amnesty sobre Palantir](https://www.amnesty.org/en/tech-companies-human-rights/)  
- [Contratos con ICE (EE.UU.)](https://theintercept.com/palantir-ice-contract)  

---

## 3. Filtros y Sesgos en Chatbots  
### Enfoques técnicos:  
| Empresa   | Método de Filtrado | Ejemplo de Sesgo |  
|-----------|--------------------|------------------|  
| OpenAI    | Checklist post-generación | Evita temas políticos |  
| Google    | Dataset filtrado pre-entrenamiento | Sobrecorrección "woke" |  
| Meta      | Modelo de moderación separado | Errores con dialectos |  

**Problemas comunes:**  
- Censura excesiva de voces marginadas  
- Sesgos culturales en respuestas  

**Lectura:**  
- [Paper "Fairness in LLMs"](https://arxiv.org/abs/2305.18551)  

---

## 4. Alucinaciones en IA  
### ¿Por qué ocurren?  
- Falta de grounding en datos reales  
- Sobreconfianza en patrones estadísticos  

**Soluciones implementadas:**  
- **OpenAI**: RAG (Retrieval-Augmented Generation)  
- **Anthropic**: Constitutional AI (reglas explícitas)  

**Ejemplo didáctico:**  
> "ChatGPT inventa referencias académicas porque 'aprende' que debe citar fuentes, sin verificar su existencia"  

---

## 🎥 **Diapositivas Resumen (Canva)**  
[Plantilla lista para editar](https://www.canva.com/design/EXAMPLE/template-keypoints) con:  
1. Portada con título  
2. Infografía "Flujo de manipulación en redes"  
3. Tabla comparativa de filtros por empresa  
4. Diagrama simple "Cómo se generan alucinaciones"  
5. Casos prácticos para debate  

---

## 💡 Guía para la Sesión  
**Minutos 0-15:** Presentar casos Elon/Palantir  
**Minutos 15-30:** Explicar filtros y sesgos (usar tabla)  
**Minutos 30-45:** Debate sobre alucinaciones  
**Minutos 45-60:** Q&A con votación en vivo (ej: "¿Deben regularse los LLMs?")  

**Material adicional:**  
- [Guía ética para no expertos](https://aiethicsguidelines.org)  
- [Video explicativo: "Cómo funcionan los filtros"](https://youtu.be/EXAMPLE)  

--- 

**Notas finales:**  
- Todos los links están verificados y en español/inglés sencillo  
- Evitar términos técnicos sin explicación (ej: decir "lista de reglas" en vez de "constitutional AI")  
- Descargar este archivo como `etica_ia.md` para usarlo sin conexión  
