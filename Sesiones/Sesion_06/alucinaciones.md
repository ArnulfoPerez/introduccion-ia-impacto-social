# ğŸ§  Alucinaciones en modelos de lenguaje: quÃ© son, por quÃ© ocurren y cÃ³mo se estÃ¡n abordando

## ğŸ“Œ Resumen ejecutivo

Las alucinaciones en modelos de lenguaje (LLMs) **no ocurren de forma completamente aleatoria**. SegÃºn investigaciones recientes, son **mÃ¡s probables** cuando:

- El prompt requiere una **respuesta larga o elaborada**.
- **No existe suficiente informaciÃ³n** en el corpus de entrenamiento.
- La informaciÃ³n disponible **contradice la premisa del prompt**.
- El tema involucra **opiniones mayoritarias errÃ³neas** o **datos poco difundidos**.

Estas afirmaciones estÃ¡n respaldadas por estudios como el de [Nerds.ai](https://www.nerds.ai/blog/alucinaciones-en-llms-que-son-por-que-ocurren-y-como-mitigarlas-en-produccion) y [Unite.AI](https://www.unite.ai/es/qu%C3%A9-son-las-alucinaciones-llm-causas-preocupaci%C3%B3n-%C3%A9tica-prevenci%C3%B3n/), que analizan cÃ³mo los LLMs generan contenido plausible pero incorrecto cuando enfrentan ambigÃ¼edad, contradicciÃ³n o falta de datos.

---

## ğŸ§  Â¿QuÃ© son las alucinaciones en IA?

Las **alucinaciones** son respuestas generadas por un modelo de lenguaje que **parecen coherentes y confiables**, pero **no estÃ¡n basadas en hechos reales**. Pueden incluir:

- Datos falsos (fechas, cifras, nombres).
- Citas inventadas.
- Narrativas que suenan verosÃ­miles pero son ficticias.

---

## ğŸ” Â¿Por quÃ© ocurren?

### Causas tÃ©cnicas

- **PredicciÃ³n probabilÃ­stica**: Los LLMs predicen la siguiente palabra, no la verdad.
- **Falta de grounding**: No tienen conexiÃ³n directa con bases de datos o el mundo real.
- **Corpus incompleto o sesgado**: Si el modelo no ha visto suficiente informaciÃ³n sobre un tema, lo â€œrellenaâ€.
- **Prompts ambiguos o contradictorios**: El modelo intenta resolver la ambigÃ¼edad con su mejor conjetura.

### Factores que aumentan el riesgo

| Factor del prompt                          | Riesgo de alucinaciÃ³n |
|--------------------------------------------|------------------------|
| Solicita respuesta larga o compleja        | Alto                   |
| Tema con poca difusiÃ³n o datos limitados   | Alto                   |
| Premisa contradictoria o errÃ³nea           | Muy alto               |
| OpiniÃ³n social dominante pero incorrecta   | Alto                   |
| Pregunta sobre eventos recientes           | Alto (si el modelo no estÃ¡ actualizado) |

---

## ğŸ§ª Estado del arte en mitigaciÃ³n

### ğŸ”§ TÃ©cnicas actuales

- **RLHF (Reinforcement Learning with Human Feedback)**: Entrenamiento por refuerzo con retroalimentaciÃ³n humana. Mejora la alineaciÃ³n del modelo con respuestas deseadas.
- **DetecciÃ³n a nivel de token**: Algoritmos que identifican tokens alucinados en tiempo real.
- **Fine-tuning con datos verificados**: Ajuste del modelo con corpus curado.
- **Guardrails y validaciÃ³n externa**: Sistemas que filtran o corrigen respuestas antes de mostrarlas.

### ğŸ§  Avances recientes

- OpenAI y DeepMind han desarrollado variantes de RLHF para reducir alucinaciones en ChatGPT y Gemini.
- Investigadores como Ilya Sutskever afirman que el refuerzo posterior puede enseÃ±ar al modelo a no alucinar.
- Nuevos modelos como Claude 3 y DeepSeek R1 incorporan mecanismos de verificaciÃ³n interna y razonamiento paso a paso.

---

## ğŸ§­ Â¿QuÃ© puede hacer el usuario?

- **Verificar fuentes**: No confiar ciegamente en respuestas generadas.
- **Usar prompts claros y especÃ­ficos**.
- **Solicitar citas o referencias**.
- **Comparar respuestas entre modelos**.

---

## ğŸ“š Referencias

- [Nerds.ai: QuÃ© son las alucinaciones y cÃ³mo mitigarlas](https://www.nerds.ai/blog/alucinaciones-en-llms-que-son-por-que-ocurren-y-como-mitigarlas-en-produccion)
- [Unite.AI: Causas y prevenciÃ³n de alucinaciones](https://www.unite.ai/es/qu%C3%A9-son-las-alucinaciones-llm-causas-preocupaci%C3%B3n-%C3%A9tica-prevenci%C3%B3n/)
- [Jucaripo: Impacto y causas de alucinaciones en IA](https://jucaripo.com/2025/06/ia-alucinaciones-causas-impacto-consejo/)

---

## ğŸ–¼ï¸ Diapositivas resumen

### Slide 1: Â¿QuÃ© son las alucinaciones?
- DefiniciÃ³n
- Ejemplos comunes

### Slide 2: Â¿Por quÃ© ocurren?
- PredicciÃ³n probabilÃ­stica
- Corpus incompleto
- Prompts ambiguos

### Slide 3: Â¿CuÃ¡ndo son mÃ¡s probables?
- Respuestas largas
- Temas contradictorios
- Opiniones sociales errÃ³neas

### Slide 4: Â¿CÃ³mo se mitigan?
- RLHF
- DetecciÃ³n token por token
- ValidaciÃ³n externa

### Slide 5: Â¿QuÃ© puede hacer el usuario?
- Verificar
- Prompting claro
- Comparar modelos

---

