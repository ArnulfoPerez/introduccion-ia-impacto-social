---
---
# Fundamentos Conceptuales del Machine Learning:  


---

## I. RegresiÃ³n: El CorazÃ³n MatemÃ¡tico del Aprendizaje  

### Â¿QuÃ© es RegresiÃ³n?  
En esencia, **encontrar la mejor curva** que explique una nube de puntos. Formalmente:  

```math  
\min_{Î¸} \sum_{i=1}^n (y_i - f(x_i; Î¸))^2
```
donde 
```math
f(x;Î¸)
```
es nuestro modelo y Î¸ sus parÃ¡metros.

#### Ejemplo intuitivo:

Ajustar una recta (regresiÃ³n lineal) a datos de precio de casas vs. metros cuadrados.

### ConexiÃ³n con Redes Neuronales
Una red neuronal es una funciÃ³n paramÃ©trica ultra-flexible:

- Capas ocultas = transformaciones no lineales
- ParÃ¡metros Î¸ = pesos sinÃ¡pticos
- Entrenar = ajustar Î¸ para minimizar error (como en regresiÃ³n, pero con gradiente descendente).

## II. ğŸ§  Overfitting

### â“ Â¿QuÃ© es?

El **overfitting** ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el **ruido y las excepciones**, en lugar de captar los **patrones generales**.

Esto significa que cuando el modelo tiene muchos parÃ¡metros (Î¸) y poca seÃ±al Ãºtil en los datos, existe gran riesgo de que aprenda ruido.

---

## ğŸ›¡ï¸ Estrategias AntÃ­doto

### 1. ğŸ“Š GestiÃ³n Inteligente de Datos

| Conjunto      | PropÃ³sito                         | % TÃ­pico    |
|---------------|-----------------------------------|-------------|
| Entrenamiento | Ajustar parÃ¡metros (Î¸)            | 60â€“70%      |
| ValidaciÃ³n    | Tunar hiperparÃ¡metros             | 15â€“20%      |
| Prueba        | EvaluaciÃ³n final (Â¡solo una vez!) | 15â€“20%      |

ğŸ” **Protocolo clave**:  
**K-fold Cross-Validation**  
â†’ Dividir datos en K bloques, usar Kâ€“1 para entrenar, 1 para validar, rotar el proceso.

---

### 2. ğŸ§® RegularizaciÃ³n

- **L1 / L2**: Penalizan parÃ¡metros grandes para evitar sobreajuste.
```math
  $$L_{\text{total}} = L_{\text{error}} + \lambda \|Î¸\|^2$$
```

Penaliza parÃ¡metros demasiado grandes

Î» controla la fuerza de la regularizaciÃ³n

### Dropout:

- Apaga neuronas aleatoriamente durante entrenamiento
- Previene dependencias excesivas entre neuronas
- **Dropout**: Apagar neuronas aleatoriamente durante el entrenamiento para obligar al modelo a generalizar.

---

## Referencias

- [AnÃ¡lisis de la regresiÃ³n](https://es.wikipedia.org/wiki/An%C3%A1lisis_de_la_regresi%C3%B3n)
